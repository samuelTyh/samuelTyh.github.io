[
  
  {
    "title": "Year end reflection: landing a new job in 2025",
    "url": "/posts/year-end-reflection-landing-a-new-job-in-2025/",
    "categories": "Career",
    "tags": "reflection, newjob, application",
    "date": "2025-12-12 14:25:00 +0100",
    "content": "ç°¡è€Œè¨€ä¹‹ï¼Œæ™‚é–“ä¾†åˆ° 2025 å¹´çš„æœ€å¾Œä¸€å€‹æœˆï¼Œæˆ‘é€™ä¸€æ•´å¹´æ¼«é•·çš„æ‰¾å·¥ä½œä¹‹è·¯çµ‚æ–¼æš«æ™‚æ‰¾åˆ°ä¸€å€‹æ£²èº«ä¹‹æ‰€ã€‚ å°æˆ‘ä¾†èªªï¼Œä¸€ä»½å·¥ä½œé™¤äº†è¦èƒ½å¸¶ä¾†æˆå°±æ„Ÿï¼Œä¹Ÿå¾—æ˜¯ä¸€å€‹èƒ½è®“æˆ‘ç·´åŠŸæ‰“æ€ªã€ç²å¾—å ±é…¬çš„åœ°æ–¹ã€‚  å‰å¹¾å¤©æˆ‘ç°¡å–®çµ±è¨ˆäº†ä¸€ä¸‹ä»Šå¹´çš„æ±‚è·è½‰æ›ç‡ï¼Œæ‰é©šè¦ºæ•¸å­—å…¶å¯¦ä¸å·®ï¼Œä½†æˆ‘çš„é«”æ„Ÿå»æ²’æœ‰æ¯”éå»å¹¾å¹´è¼•é¬†å¤šå°‘ã€‚                 Apply       FirstContact       HiringManager       TechnicalRound       Onsite(Final)       Offer                       ~81       25       16       13       3       2           å…¶ä¸­ Apply åŒ…å«ä¸»å‹•æŠ•éèˆ‡ Recruiter è¯ç¹«ã€‚æˆ‘æœ€æ„å¤–çš„æ˜¯ï¼Œä»Šå¹´å±…ç„¶ä¸åˆ° 100 å€‹ applicationsã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼Œ2020 é‚£æ¬¡æ±‚è·å®Œå…¨æ˜¯åœ°ç„ï¼šæˆ‘æ‡‰è©²æŠ•äº†è¶…é 300 ä»½å±¥æ­·ï¼Œæœ€å¾Œåªæ‹¿åˆ° 1 å€‹ offerã€‚  å›é ­çœ‹é€™äº›æ•¸å­—ï¼Œå¯ä»¥æ„Ÿå—åˆ°æŸç¨®æˆé•·çš„è»Œè·¡â”€â”€ å¾ã€Œç‹‚æŠ• 300 ä»½å±¥æ­·åªæ›åˆ°ä¸€å€‹æ©Ÿæœƒã€çš„è¡æ“Šï¼Œåˆ°å¦‚ä»Šä»¥ç›¸å°å°‘çš„æŠ•éé‡æ›åˆ°æ›´å¤šå¾ŒçºŒé¢è©¦ã€‚ ä½†è€å¯¦èªªï¼Œæƒ…ç·’ä¸Šçš„ç–²æ†Šä¸¦ä¸æœƒå› ç‚º conversion è®Šå¥½å°±è‡ªå‹•æ¶ˆå¤±ã€‚  ä»Šå¹´çš„æ±‚è·é€±æœŸå¹¾ä¹æ˜¯å›ºå®šç¨‹å¼ï¼š æ•´ç†å±¥æ­· â†’ åšå…¬å¸åŠŸèª²ã€ç·´ç¿’è¡Œç‚ºé¡Œ â†’ æ‹¿åˆ° first contact â†’ è‡ªæˆ‘æ‡·ç–‘ â†’ technical interview â†’ é©šéšªåº¦æ—¥ â†’ ç­‰é€šçŸ¥ â†’ å†æ¬¡è‡ªæˆ‘æ‡·ç–‘ã€‚ æ±‚è·çœŸçš„å¾ˆåƒæ˜¯ä¸€å ´å¿ƒç†è€åŠ›è³½ï¼ŒæŠ€è¡“åªæ˜¯é–€ç¥¨ï¼Œå¿ƒæ…‹æ‰æ˜¯èƒ½ä¸èƒ½æ’ä¸‹å»çš„é—œéµã€‚  å°¤å…¶æ˜¯ culture fitã€‚ ç•¶ä½ è¬›å®Œä¸€æ®µç­”æ¡ˆï¼Œé¢è©¦å®˜çš„çœ¼ç¥æ˜¯äº®çš„é‚„æ˜¯æš—çš„ï¼Œå…¶å¯¦ä½ è‡ªå·±éƒ½æ„Ÿè¦ºå¾—åˆ°ã€‚ ä»Šå¹´æˆ‘åƒéå¹¾æ¬¡å¤§è™§ï¼š ä¸æ˜¯å°æ–¹åå¥½ super outgoing æˆ–å¼·å‹¢ä¸»å°å‹çš„å·¥ç¨‹å¸«ï¼Œå°±æ˜¯æˆ‘å°ä»–å€‘çš„å·¥ä½œç¯€å¥ä¸å¤ èˆˆå¥®ã€‚ é€™ç¨® mismatch ä½ ç„¡æ³•å¼·æ±‚ï¼Œåªèƒ½æ¥å—ã€Œæœ‰äº›é–€ä¸æ˜¯ä½ çš„é–€ã€ã€‚  è€Œæˆ‘æœ€å¤§çš„ç“¶é¢ˆä»ç„¶æ˜¯ whiteboard codingã€‚ æˆ‘çŸ¥é“å°è¨±å¤šè³‡æ·±å·¥ç¨‹å¸«ä¾†èªªï¼Œé€™å·²ç¶“æ˜¯åŸºæœ¬é«”èƒ½ï¼Œä½†æˆ‘ä¸€ååˆ°ç™½æ¿å‰ï¼Œè…¦è¢‹å°±æœƒç¬é–“çŸ­è·¯ã€‚ å€’ä¸æ˜¯è§£ä¸å‡ºä¾†ï¼Œè€Œæ˜¯èªè¨€åˆ‡æ›ï¼ˆè‹±æ–‡ï¼‰ï¼‹ç¾å ´æ€è€ƒå£“åŠ›è®“å¿ƒè·³ç›´æ¥é£†åˆ° 190ã€‚ ä»Šå¹´è¡¨ç¾æ¯”è¼ƒå¥½çš„ roundï¼Œå¤§å¤šæ˜¯ take-home assignment æˆ– case studyï¼Œå› ç‚ºé‚£èƒ½å±•ç¾æˆ‘çœŸå¯¦çš„å·¥ä½œæ–¹å¼ï¼Œè€Œä¸æ˜¯è¡¨æ¼”æ€§çš„ã€Œå³èˆˆæ¼”ç®—æ³•ç§€ã€ã€‚  ä½†é€™äº›å¼±é»ï¼Œæˆ‘ä¸æœƒå†é€ƒé¿ã€‚ å¾ˆæ˜é¡¯ï¼Œæˆ‘æƒ³å»çš„å…¬å¸ã€æƒ³è¦çš„è·ä½ï¼Œéƒ½éœ€è¦æˆ‘æŠŠé€™äº›èƒ½åŠ›è£œèµ·ä¾†ã€‚  2025 çš„é€™ä»½ offer å°æˆ‘ä¾†èªªä¸æ˜¯çµ‚é»ï¼Œè€Œæ˜¯ checkpointã€‚ å®ƒæé†’æˆ‘ï¼š ã€Œä½ æ²’æœ‰ä¸åˆæ ¼ï¼Œä½ åªæ˜¯é‚„æ²’æº–å‚™åˆ°èƒ½è®“æ‰€æœ‰é–€éƒ½ç‚ºä½ æ‰“é–‹ã€‚ã€  æ¥ä¸‹ä¾†ï¼Œæˆ‘æ‰“ç®—æ›´æœ‰ç³»çµ±åœ°èª¿æ•´å¹¾ä»¶äº‹ï¼š  è‹±æ–‡å£èªªï¼šå›ºå®šç·´ç¿’é »ç‡ï¼Œå°¤å…¶æ˜¯ technical explanationã€‚  whiteboarding å¿ƒæ…‹èˆ‡ç¯€å¥ï¼šç·´ç¿’ verbal thinkingï¼Œè€Œä¸æ˜¯ç›´æ¥è¡å»å¯« codeã€‚  æ–‡åŒ–é¢è©¦ç­–ç•¥ï¼šä¸å†è¢«å‹•å›ç­”ï¼Œè€Œæ˜¯ä¸»å‹•å±•ç¤ºæˆ‘å¦‚ä½•åˆä½œã€å¦‚ä½•è§£æ±ºè¡çªã€‚  data engineering fundamentalsï¼šæŠŠé‚£äº›ã€ŒçŸ¥é“ä½†è¬›ä¸é †ã€çš„æ¦‚å¿µå¾¹åº•æ‰“åº•ã€‚  é€™ä¸€å¹´èµ°å¾—æ…¢ï¼Œä½†æ¯ä¸€æ­¥éƒ½å¾€å‰ã€‚ æ‰¾åˆ°å·¥ä½œä¸æ˜¯å¹¸é‹ï¼Œè€Œæ˜¯ä¸€æ¬¡æ¬¡æŠŠé–€æ¨é–‹çš„çµæœã€‚  å¦‚æœ 2020 æ˜¯ä¸€æ®µæ±‚è·åœ°ç„ï¼Œé‚£ 2025 æ›´åƒæ˜¯ä¸€å ´æ±‚è·ä¿®è¡Œã€‚  å›é ­çœ‹ï¼Œæˆ‘å…¶å¯¦æ²’æœ‰è‡ªå·±æƒ³åƒå¾—é‚£éº¼å …å¼·ã€‚ æœ‰æ™‚å€™æˆ‘æœƒæ‡·ç–‘è‡ªå·±æ˜¯ä¸æ˜¯æµªè²»äº†å¤ªå¤šæ™‚é–“ï¼š æ˜æ˜çŸ¥é“è©²è£œå¼·ä»€éº¼ï¼Œä¹Ÿæ˜ç™½å¸‚å ´ç«¶çˆ­æ›´æ¿€çƒˆï¼Œä½†åœ¨å£“åŠ›æœ€é‡çš„æ™‚å€™ï¼Œæˆ‘ä»ç„¶é¸æ“‡é€ƒé¿ã€‚ è€Œä»Šå¹´æœ€å¤§çš„æˆé•·ï¼Œåè€Œæ˜¯æˆ‘çµ‚æ–¼èƒ½å¦ç„¶æ‰¿èªâ”€â”€ ã€Œæˆ‘æœƒç´¯ï¼Œæˆ‘æœƒæ€•ï¼Œæˆ‘æœƒåœä¸‹ä¾†ã€‚ã€  ä½†åœä¸‹ä¾†ä¸¦æ²’æœ‰è®“æˆ‘è®Šæˆå»¢ç‰©ï¼Œåè€Œè®“æˆ‘é‡æ–°æ‰¾å›è‡ªå·±ã€‚ å» Bali çš„é‚£å››é€±ï¼Œè®“æˆ‘ç¬¬ä¸€æ¬¡è¦ºå¾—å¤±æ¥­ä¸æ˜¯ä¸–ç•Œæœ«æ—¥ï¼› å¤å¤©é™ªä¼´å®¶äººçš„é‚£ä¸€å€‹æœˆï¼Œä¹Ÿè®“æˆ‘æ˜ç™½æœ‰äº›é—œä¿‚æ¯”ä»»ä½• offer éƒ½é‡è¦ã€‚ é‚£äº›æ™‚é–“ä¸æ˜¯ç©ºç™½ï¼Œè€Œæ˜¯æˆ‘æ’éé€™ä¸€å¹´çš„èƒ½é‡ä¾†æºã€‚  ç•¶ç„¶ï¼Œä¸­é–“æˆ‘ä¹Ÿè’å»¢äº†ä¸å°‘ç·´ç¿’ã€æ¨æ‰ä¸€äº›æ©Ÿæœƒï¼Œç”šè‡³æ•´æ•´å…©å€‹æœˆå¹¾ä¹ä¸æ•¢é¢å°é¢è©¦æº–å‚™ã€‚ æˆ‘ä¸å¦èªé€™äº›ï¼Œä½†æˆ‘ä¹Ÿä¸å†è²¬æ€ªè‡ªå·±ã€‚ å¦‚æœä¸æ˜¯é‚£äº›æ··äº‚èˆ‡åœæ»¯ï¼Œæˆ‘ä¹Ÿä¸æœƒä»¥å¦‚ä»Šé€™ç¨®ç›¸å°æ¸…é†’çš„ç‹€æ…‹èµ°åˆ°å¹´åº•ã€‚  æˆ‘ä¹Ÿå¾ˆæ„Ÿè¬ä»Šå¹´é‚£äº›æ²’æœ‰èµ°ä¸‹å»çš„æµç¨‹ã€è¢«æ‹’çµ•çš„é¢è©¦ã€å†·æ‰çš„ recruiterã€‚ æ¯ä¸€æ¬¡ã€Œæ²’ç·£åˆ†ã€éƒ½æŠŠæˆ‘æ¨å›è‡ªå·±çš„è»Œé“ï¼Œè®“æˆ‘æ›´æ¸…æ¥šä»€éº¼è©²è£œã€ä»€éº¼è©²æ”¾ã€ä»€éº¼æ‰æ˜¯æˆ‘çœŸæ­£æƒ³è¦çš„ã€‚  æ‰€ä»¥ç¾åœ¨çš„æˆ‘ï¼Œä¸å†æŠŠæ±‚è·ç•¶æˆä¸€å ´ã€Œæ‹¼å‘½è­‰æ˜è‡ªå·±å¤ å¥½ã€çš„æ¯”è³½ï¼Œ è€Œæ¯”è¼ƒåƒæ˜¯ä¸€æ®µã€ŒæŒçºŒæ ¡æº–æ–¹å‘ã€çš„æ—…ç¨‹ã€‚ æ¯ä¸€æ¬¡å¤±æ•—éƒ½è®“æˆ‘æ›´é è¿‘é©åˆè‡ªå·±çš„ç”Ÿæ´»èˆ‡å·¥ä½œï¼Œ è€Œæ¯ä¸€æ¬¡æˆåŠŸä¹Ÿæé†’æˆ‘â”€â”€æˆ‘ä¸æ˜¯å¾é›¶é–‹å§‹ï¼Œæˆ‘åªæ˜¯ç¹¼çºŒå‰é€²ã€‚"
  },
  
  {
    "title": "Free n8n Hosting: Leveraging Hugging Face Spaces and Supabase for Persistent Workflow Automation",
    "url": "/posts/free-n8n-hosting-leveraging-hugging-face-spaces-and-supabase-for-persistent-workflow-automation/",
    "categories": "Learning Journey",
    "tags": "n8n, automation, workflow, huggingface, supabase",
    "date": "2025-06-02 20:52:00 +0200",
    "content": "Introduction  n8n, the fair-code workflow automation platform, has transformed how developers and businesses automate their processes. While self-hosting n8n typically requires dedicated infrastructure, this guide demonstrates an innovative approach: hosting n8n entirely free using Hugging Face Spaces for compute and Supabase for persistent storage.  This architecture addresses a critical limitation of Hugging Faceâ€™s free tierâ€”ephemeral storageâ€”by offloading workflow persistence to Supabaseâ€™s generous free database tier. The result is a production-ready n8n instance without infrastructure costs, perfect for individuals, small teams, or proof-of-concept deployments.  Architecture Overview  The solution leverages two complementary platforms:  Hugging Face Spaces provides:    Free Docker container hosting   2 vCPU and 16GB RAM (free tier)   Public HTTPS endpoint   Automatic container management   Supabase delivers:    PostgreSQL database (500MB free tier)   Row-level security   Real-time subscriptions   Built-in authentication   This combination circumvents Hugging Faceâ€™s ephemeral storage limitation while maintaining the performance characteristics needed for reliable workflow automation.  Prerequisites  Before beginning, ensure you have:    A Hugging Face account (free signup at huggingface.co)   A Supabase account (free signup at supabase.com)   Basic familiarity with Docker and environment variables   Git installed locally   Step 1: Set Up Supabase Database  First, create the PostgreSQL backend that will store your n8n workflows, credentials, and execution history.  Create a New Supabase Project     Log into your Supabase dashboard   Click â€œNew projectâ€   Configure your project:            Name: n8n-backend       Database Password: Generate a strong password (save this!)       Region: Choose the closest to your primary users           Wait for project provisioning (typically 2-3 minutes)   Configure Database Schema  n8n requires specific database tables and permissions. Navigate to the SQL editor in your Supabase dashboard and execute:  -- Create n8n schema CREATE SCHEMA IF NOT EXISTS n8n;  -- Grant permissions to authenticated users GRANT ALL ON SCHEMA n8n TO postgres; GRANT ALL ON ALL TABLES IN SCHEMA n8n TO postgres; GRANT ALL ON ALL SEQUENCES IN SCHEMA n8n TO postgres;  -- Enable UUID extension for n8n CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";   Retrieve Connection Details  From your Supabase project settings, collect:    Host: Found under Settings â†’ Database   Port: Typically 5432   Database name: Usually â€˜postgresâ€™   User: â€˜postgresâ€™   Password: The password you created earlier   Your connection string will look like: postgresql://postgres:[YOUR-PASSWORD]@[YOUR-PROJECT-REF].supabase.co:5432/postgres   Step 2: Create Hugging Face Space  Now, set up the compute environment for n8n.  Initialize Space Repository     Navigate to huggingface.co/spaces   Click â€œCreate new Spaceâ€   Configure:            Space name: n8n-workflow-automation       SDK: Docker       Visibility: Public (required for free tier)           Clone the repository locally:     git clone https://huggingface.co/spaces/[YOUR-USERNAME]/n8n-workflow-automation cd n8n-workflow-automation           Create Dockerfile  Create a Dockerfile that configures n8n with Supabase integration:  FROM n8nio/n8n:latest  # Install PostgreSQL client for better database compatibility USER root RUN apk add --no-cache postgresql-client  # Create data directory with proper permissions RUN mkdir -p /home/node/.n8n &amp;&amp; \\     chown -R node:node /home/node/.n8n  USER node  # Set working directory WORKDIR /home/node  # Expose n8n port EXPOSE 5678  # Health check HEALTHCHECK --interval=30s --timeout=5s --start-period=30s \\     CMD curl -f http://localhost:5678/healthz || exit 1  # Start n8n CMD [\"n8n\", \"start\"]   Configure Environment Variables  Create a .env file template (donâ€™t commit actual values):  # Database Configuration DB_TYPE=postgresdb DB_POSTGRESDB_HOST=your-project.supabase.co DB_POSTGRESDB_PORT=5432 DB_POSTGRESDB_DATABASE=postgres DB_POSTGRESDB_USER=postgres DB_POSTGRESDB_PASSWORD=your-password DB_POSTGRESDB_SCHEMA=n8n  # n8n Configuration N8N_BASIC_AUTH_ACTIVE=true N8N_BASIC_AUTH_USER=admin N8N_BASIC_AUTH_PASSWORD=your-admin-password N8N_HOST=0.0.0.0 N8N_PORT=5678 N8N_PROTOCOL=https WEBHOOK_URL=https://your-space.hf.space  # Execution Configuration EXECUTIONS_MODE=regular EXECUTIONS_PROCESS=main N8N_METRICS=false  # Security N8N_ENCRYPTION_KEY=your-32-char-encryption-key   Create Space Configuration  Add a README.md with YAML frontmatter:  --- title: n8n Workflow Automation emoji: ğŸ”„ colorFrom: blue colorTo: purple sdk: docker pinned: false ---  # n8n Workflow Automation Platform  This Space hosts a fully functional n8n instance with persistent storage via Supabase.  ## Features - Complete n8n workflow automation - Persistent workflow storage - Secure credential management - Webhook support - API integrations  ## Access Visit the Space URL and log in with the configured credentials.   Step 3: Deploy and Configure  Set Hugging Face Secrets  In your Space settings, add these secrets:    Navigate to Settings â†’ Variables and secrets   Add each environment variable from your .env file   Mark sensitive values (passwords, keys) as â€œSecretâ€   Initial Deployment  Push your configuration:  git add . git commit -m \"Initial n8n configuration\" git push origin main   Hugging Face will automatically build and deploy your container. Monitor the logs for any issues.  Verify Database Connection  Once deployed, access your n8n instance and verify:    Navigate to https://[your-space].hf.space   Log in with your basic auth credentials   Create a test workflow   Restart the Space (Settings â†’ Restart)   Confirm your workflow persists   Step 4: Configure n8n for Production Use  Enable Webhook URLs  For webhook-triggered workflows, configure the public URL:     In n8n settings, set Webhook URL to your Space URL   Test with a simple webhook workflow:     {   \"nodes\": [  {    \"name\": \"Webhook\",    \"type\": \"n8n-nodes-base.webhook\",    \"position\": [250, 300],    \"webhookId\": \"test-webhook\",    \"parameters\": {      \"path\": \"test\",      \"responseMode\": \"onReceived\",      \"responseData\": \"allEntries\"    }  }   ] }           Set Up Credentials Encryption  n8n encrypts stored credentials using the N8N_ENCRYPTION_KEY. Generate a secure key:  openssl rand -hex 32   Update this in your Space secrets to ensure credential security.  Configure Execution Retention  To manage database growth within Supabaseâ€™s free tier, configure execution pruning:  EXECUTIONS_DATA_SAVE_ON_ERROR=all EXECUTIONS_DATA_SAVE_ON_SUCCESS=none EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS=true EXECUTIONS_DATA_MAX_AGE=168  # 7 days   Optimization and Best Practices  Performance Tuning     Connection Pooling: Configure n8nâ€™s database connection pool:     DB_POSTGRESDB_POOL_MIN=2 DB_POSTGRESDB_POOL_MAX=10                Memory Management: Monitor Space metrics and adjust workflow complexity accordingly      Webhook Response Times: Keep webhook workflows lightweight to avoid timeouts   Security Considerations     Access Control: Always enable basic authentication or implement OAuth   Network Security: Use Supabaseâ€™s connection pooler for additional security   Credential Rotation: Regularly update passwords and API keys   Audit Logging: Enable n8nâ€™s audit logs for compliance   Backup Strategy  Implement regular backups despite the free tier limitations:  # Weekly backup script (run locally) pg_dump -h your-project.supabase.co -U postgres -d postgres -n n8n &gt; backup_$(date +%Y%m%d).sql   Advantages for n8n Beginners  This setup offers several compelling benefits:  Zero Infrastructure Costs    No server hosting fees   No database hosting costs   No domain or SSL certificate expenses   Perfect for learning and experimentation   Production-Ready Features    HTTPS endpoint provided automatically   Database backups via Supabase   Scalable to paid tiers when needed   Professional deployment practices   Learning Opportunities    Understand containerized deployments   Practice with PostgreSQL databases   Explore webhook integrations   Build real automation workflows   Easy Migration Path When ready to scale:    Export workflows from n8n   Backup Supabase database   Deploy to any cloud provider   Import data and continue   Troubleshooting Common Issues  Space Sleeping Hugging Face Spaces sleep after inactivity. Solutions:    Use external monitoring to ping your Space   Implement a scheduled workflow that runs regularly   Upgrade to a paid Space for always-on availability   Database Connection Errors If n8n canâ€™t connect to Supabase:    Verify connection string formatting   Check Supabase connection limits   Ensure proper SSL mode configuration   Review Space logs for detailed errors   Webhook Timeouts For long-running webhooks:    Implement async processing patterns   Use n8nâ€™s â€œRespond to Webhookâ€ node   Break complex workflows into smaller pieces   Future Enhancements  As you grow comfortable with this setup, consider:     Custom Nodes: Build and deploy custom n8n nodes   Multi-Instance: Run multiple n8n instances with shared database   Advanced Monitoring: Integrate with Supabaseâ€™s real-time features   API Gateway: Add rate limiting and authentication layers   Conclusion  Hosting n8n on Hugging Face Spaces with Supabase backend represents a paradigm shift in accessible workflow automation. This architecture eliminates traditional barriers to entry while maintaining professional-grade capabilities.  For beginners, it provides a risk-free environment to explore automation possibilities. For experienced users, it offers a viable production platform for non-critical workflows. As the ecosystem evolves, expect tighter integrations and enhanced capabilities that further democratize workflow automation.  The convergence of specialized platforms like Hugging Face and Supabase demonstrates the future of composable infrastructureâ€”where developers assemble best-in-class services rather than managing monolithic deployments. This approach not only reduces operational overhead but accelerates innovation by allowing focus on business logic rather than infrastructure management.  Start building your automation workflows today, and join the growing community leveraging free-tier infrastructure for real-world solutions."
  },
  
  {
    "title": "Terraform for Data Engineers: Automating Your Data Infrastructure",
    "url": "/posts/terraform-for-data-engineers-automating-your-data-infrastructure/",
    "categories": "Learning Journey",
    "tags": "terraform, devops, hashicorp, iac",
    "date": "2025-05-06 21:34:00 +0200",
    "content": "As a data engineer, weâ€™re all too familiar with the pain of manually provisioning data processing resources, dealing with inconsistent environments, and the nightmare of trying to recreate a failed data pipeline. Enter Terraform â€“ a powerful tool that lets us define our entire data infrastructure as code, making it versionable, repeatable, and automated.  What is Terraform?  Terraform is an infrastructure as code tool that allows you to define, provision, and manage cloud resources across providers like AWS, GCP, and Azure using a simple, declarative language. Instead of clicking through console UIs or writing custom scripts, you write configuration files that describe your desired infrastructure state, and Terraform makes it happen.  What makes Terraform particularly valuable for data engineers is its ability to provision and manage all the components of modern data platforms â€“ from storage and compute resources to data warehouses, ETL services, and analytics tools â€“ using a consistent workflow.  How Terraform Fits into Data Engineering  As data infrastructure grows more complex, crossing multiple cloud platforms and including dozens of specialized services, the old approach of manual provisioning becomes untenable. Terraform addresses this by:     Automating repetitive tasks - Set up data lakes, data warehouses, and compute clusters with code rather than click-ops   Standardizing environments - Ensure development, staging, and production environments are identical   Enabling infrastructure evolution - Version control your data infrastructure alongside your code   Supporting collaboration - Let team members understand and contribute to infrastructure changes   Terraform Basics for Data Engineers  Terraform uses HashiCorp Configuration Language (HCL) for its configuration files. Hereâ€™s a simple example showing how to set up an AWS S3 bucket for data lake storage:  provider \"aws\" {   region = \"us-west-2\" }  # Create an S3 bucket for our data lake resource \"aws_s3_bucket\" \"data_lake\" {   bucket = \"my-company-data-lake\"      tags = {     Environment = \"production\"     Department  = \"data-engineering\"   } }  # Set up bucket for analytics results resource \"aws_s3_bucket\" \"analytics_results\" {   bucket = \"my-company-analytics-results\"      tags = {     Environment = \"production\"     Department  = \"data-engineering\"   } }   The Basic Terraform Workflow  Working with Terraform follows a straightforward process:     Write your configuration in .tf files   Init your project with terraform init to download providers   Plan changes with terraform plan to see what will be created/modified   Apply with terraform apply to create the resources   Destroy with terraform destroy when youâ€™re done   This workflow is particularly useful for data projects where you might need to spin up temporary analysis environments or test new pipeline architectures without committing to permanent infrastructure changes.  Data Engineering Use Cases for Terraform  Letâ€™s dive into specific ways Terraform can solve your data engineering challenges:  1. Cloud Data Warehouse Provisioning  Setting up data warehouses like Redshift, BigQuery, or Snowflake requires numerous configuration choices. With Terraform, you can define these settings as code:  resource \"aws_redshift_cluster\" \"analytics_warehouse\" {   cluster_identifier  = \"analytics-warehouse\"   database_name       = \"analytics\"   master_username     = var.redshift_admin_user   master_password     = var.redshift_admin_password   node_type           = \"dc2.large\"   cluster_type        = \"multi-node\"   number_of_nodes     = 3      # Enable encryption and logging   encrypted           = true   enhanced_vpc_routing = true   logging {     enable            = true     bucket_name       = aws_s3_bucket.redshift_logs.bucket     s3_key_prefix     = \"redshift-logs/\"   } }   This approach enables you to:    Version control your warehouse configuration   Easily replicate the setup in development/testing environments   Automate warehouse scaling based on workload patterns   2. Data Lake Infrastructure  Modern data lakes involve many components beyond just storage. Terraform lets you provision the entire stack:  # S3 storage with proper partitioning setup resource \"aws_s3_bucket\" \"data_lake\" {   bucket = \"company-data-lake\" }  # Configure Glue Catalog for data discovery resource \"aws_glue_catalog_database\" \"data_catalog\" {   name = \"data_lake_catalog\" }  # Set up partitions and tables resource \"aws_glue_crawler\" \"data_crawler\" {   name          = \"data-lake-crawler\"   role          = aws_iam_role.glue_role.arn   database_name = aws_glue_catalog_database.data_catalog.name      s3_target {     path = \"s3://${aws_s3_bucket.data_lake.bucket}/raw-data/\"   }      schedule = \"cron(0 */12 * * ? *)\" }  # Add Athena workgroup for SQL queries resource \"aws_athena_workgroup\" \"analytics\" {   name = \"data-engineering\"      configuration {     result_configuration {       output_location = \"s3://${aws_s3_bucket.query_results.bucket}/athena-results/\"     }   } }   3. Streaming Data Infrastructure  Data engineers often need to set up real-time data processing pipelines. Terraform makes this easier by managing the complete infrastructure:  # Kafka cluster on MSK resource \"aws_msk_cluster\" \"event_streaming\" {   cluster_name           = \"data-events-stream\"   kafka_version          = \"2.8.1\"   number_of_broker_nodes = 3      broker_node_group_info {     instance_type   = \"kafka.m5.large\"     client_subnets  = var.private_subnets     security_groups = [aws_security_group.kafka_sg.id]     storage_info {       ebs_storage_info {         volume_size = 1000       }     }   } }  # Kinesis Firehose for stream delivery to S3 resource \"aws_kinesis_firehose_delivery_stream\" \"event_delivery\" {   name        = \"event-delivery-stream\"   destination = \"extended_s3\"      extended_s3_configuration {     role_arn   = aws_iam_role.firehose_role.arn     bucket_arn = aws_s3_bucket.data_lake.arn     prefix     = \"streaming-events/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/\"     buffer_interval = 60     buffer_size     = 64   } }   4. Compute Resources for Data Processing  Spin up and manage compute resources for data transformation jobs:  # EMR cluster for Spark processing resource \"aws_emr_cluster\" \"data_processing\" {   name          = \"data-processing-cluster\"   release_label = \"emr-6.5.0\"   applications  = [\"Spark\", \"Hive\", \"Presto\"]      ec2_attributes {     subnet_id                         = var.subnet_id     emr_managed_master_security_group = aws_security_group.emr_master.id     emr_managed_slave_security_group  = aws_security_group.emr_slave.id     instance_profile                  = aws_iam_instance_profile.emr_profile.arn   }      master_instance_group {     instance_type = \"m5.xlarge\"   }      core_instance_group {     instance_type  = \"m5.xlarge\"     instance_count = 4   }      configurations_json = &lt;&lt;EOF     [       {         \"Classification\": \"spark\",         \"Properties\": {           \"maximizeResourceAllocation\": \"true\"         }       }     ]   EOF }   5. Managed Airflow for Orchestration  Set up a fully managed Apache Airflow environment:  resource \"aws_mwaa_environment\" \"data_orchestration\" {   name               = \"data-pipeline-orchestrator\"   airflow_version    = \"2.5.1\"   source_bucket_arn  = aws_s3_bucket.airflow_assets.arn   dag_s3_path        = \"dags/\"      execution_role_arn = aws_iam_role.mwaa_execution.arn      network_configuration {     security_group_ids = [aws_security_group.mwaa_sg.id]     subnet_ids         = var.private_subnets   }      logging_configuration {     dag_processing_logs {       enabled   = true       log_level = \"INFO\"     }     scheduler_logs {       enabled   = true       log_level = \"INFO\"     }     webserver_logs {       enabled   = true       log_level = \"INFO\"     }     worker_logs {       enabled   = true       log_level = \"INFO\"     }   }      environment_class = \"mw1.medium\"   min_workers       = 2   max_workers       = 5 }   Practical Terraform Tips for Data Engineers  Creating Reusable Data Infrastructure Modules  One of Terraformâ€™s most powerful features is modularity. You can create reusable modules for common data infrastructure patterns:  # Example module usage module \"data_warehouse\" {   source = \"./modules/redshift-warehouse\"      cluster_name = \"analytics-production\"   node_count   = 4   node_type    = \"dc2.large\"   vpc_id       = module.vpc.vpc_id   subnet_ids   = module.vpc.private_subnets }  module \"data_lake\" {   source = \"./modules/s3-data-lake\"      bucket_name  = \"company-data-lake-${var.environment}\"   enable_versioning = true   lifecycle_rules = var.storage_lifecycle_rules }   Managing Secrets for Data Connections  Handling credentials for databases, warehouses, and APIs is a common challenge. Terraform integrates with secrets management services:  # Use AWS Secrets Manager for database credentials resource \"aws_secretsmanager_secret\" \"warehouse_creds\" {   name = \"data/warehouse/admin\" }  resource \"aws_secretsmanager_secret_version\" \"warehouse_creds\" {   secret_id     = aws_secretsmanager_secret.warehouse_creds.id   secret_string = jsonencode({     username = var.admin_username     password = var.admin_password   }) }  # Reference in your Redshift configuration resource \"aws_redshift_cluster\" \"warehouse\" {   # ... other configuration   master_username = jsondecode(data.aws_secretsmanager_secret_version.warehouse_creds.secret_string)[\"username\"]   master_password = jsondecode(data.aws_secretsmanager_secret_version.warehouse_creds.secret_string)[\"password\"] }   Testing Data Infrastructure Changes  Before applying changes to production systems, you can validate them:  # Validate syntax and structure terraform validate  # Check formatting terraform fmt -check  # See what will change before applying terraform plan -out=changes.plan  # Apply the validated changes terraform apply changes.plan   Integration with CI/CD for Data Projects  Integrate Terraform with your existing CI/CD pipelines to automate infrastructure updates alongside data pipeline code:  # Example GitHub Actions workflow name: Deploy Data Infrastructure  on:   push:     branches: [main]     paths:       - 'terraform/**'       - '.github/workflows/terraform.yml'  jobs:   terraform:     runs-on: ubuntu-latest     steps:       - uses: actions/checkout@v3              - name: Setup Terraform         uses: hashicorp/setup-terraform@v2              - name: Terraform Init         run: terraform init         working-directory: ./terraform              - name: Terraform Plan         run: terraform plan -out=tfplan         working-directory: ./terraform              - name: Terraform Apply         if: github.ref == 'refs/heads/main'         run: terraform apply -auto-approve tfplan         working-directory: ./terraform   Getting Started as a Data Engineer  Ready to bring infrastructure as code to your data engineering practice? Hereâ€™s how to begin:     Start small - Automate a single component of your data platform first, like an S3 bucket or Redshift cluster   Use existing modules - Explore the Terraform Registry for pre-built data infrastructure modules   Adopt gradually - You donâ€™t need to migrate everything at once; Terraform can manage resources alongside manually created ones   Version control - Store your Terraform files in the same repository as your data pipeline code   Collaborate - Share your Terraform configurations with your team to build a consistent approach   Conclusion  For data engineers, Terraform isnâ€™t just another tool â€“ itâ€™s a fundamental shift in how we work with infrastructure. By codifying our data platform, we eliminate manual errors, enable repeatable deployments, and build a foundation for continuous evolution.  Whether weâ€™re running data workloads on AWS, GCP, Azure, or across multiple clouds, Terraform provides a consistent interface to provision and manage the entire stack. This allows us to focus on what matters most: building robust data pipelines and extracting value from our organizationâ€™s data.  The initial investment in learning Terraform pays dividends in reduced complexity, greater reliability, and the ability to scale our data infrastructure alongside our growing data needs."
  },
  
  {
    "title": "Building a Production-Ready Data Pipeline with Airflow and dbt",
    "url": "/posts/building-a-production-ready-data-pipeline-with-airflow-and-dbt/",
    "categories": "Data Engineering",
    "tags": "python, postgres, airflow, dbt, etl",
    "date": "2025-04-28 14:30:00 +0200",
    "content": "In todayâ€™s data-driven business landscape, transforming raw operational data into actionable insights requires robust, scalable data pipelines. I recently designed and implemented a comprehensive data engineering solution for sales analytics that bridges raw transactional data to dimensional models. This post walks through the architecture, implementation decisions, and practical patterns that you can apply to your own data engineering projects.  The Challenge: From Sales Transactions to Analytics  The core challenge was to build a pipeline that would:     Ingest sales transaction data from multiple CSV sources   Handle common data quality issues (missing values, inconsistent formats)   Transform raw data into a dimensional model for analytics   Implement incremental processing for efficiency   Ensure reliability with comprehensive testing and error handling   Architecture Design: Medallion Pattern Implementation  After evaluating several architectural approaches, I implemented a medallion architecture (also called multi-hop architecture), which organizes data through progressive refinement stages:  Raw CSV Data â†’ Bronze Layer (raw.sales) â†’ Silver Layer (transformed.dim_*) â†’ Gold Layer (analytics.fact_sales)   This layered approach provides several key advantages:    Clear separation of concerns   Progressive data quality improvement   Complete data lineage traceability   Flexibility to rebuild downstream layers without re-ingesting source data   Database Schema Design  I designed the following schema structure:     Bronze Layer: Raw data storage with original values preserved            raw.sales: Original CSV data with added metadata columns           Silver Layer: Cleaned and transformed dimensional models            transformed.dim_product: Product information       transformed.dim_retailer: Retailer information       transformed.dim_location: Location information       transformed.dim_channel: Sales channel information       transformed.dim_date: Date dimension with hierarchies       transformed.fact_sales: Sales fact table with foreign keys and measures           Gold Layer: Analytics-ready views and aggregates            analytics.dim_*: Analytics-ready dimension views       analytics.fact_sales: Optimized analytical fact table           Implementation: Core Components  1. Data Ingestion Module  The ingestion component follows a â€œvalidate-first, then cleanâ€ pattern, which provides better visibility into data quality issues:  def main(file_path):     \"\"\"     Main function to process and load data.     New flow: Validate first, then clean only records that need cleaning.     \"\"\"     logger.info(f\"Starting data ingestion process for {file_path}\")          try:         # Read the CSV file         logger.info(f\"Reading file: {file_path}\")         df = pd.read_csv(file_path)         logger.info(f\"Successfully read {len(df)} records from {file_path}\")                  # First validate the data as-is         is_valid, invalid_indices = validate_data(df)                  if not is_valid:             logger.info(\"Data validation failed. Applying cleaning steps...\")             # Apply cleaning only after validation fails             df = CleanData.apply_all_cleaners(df)                          # Re-validate after cleaning             is_valid, invalid_indices = validate_data(df)             if not is_valid:                 logger.warning(\"Data still contains invalid records after cleaning. Filtering them out.\")                 df = filter_invalid_records(df, invalid_indices)             else:                 logger.info(\"Data cleaning resolved all validation issues.\")         else:             logger.info(\"Data passed validation without cleaning.\")                  # Check for duplicates         deduplicated_df = detect_duplicates(df)                  # Only proceed with loading if we have records         if len(deduplicated_df) &gt; 0:             # Get database connection string             connection_string = get_db_connection_string()                          # Load to raw schema             records_loaded = load_to_raw(deduplicated_df, connection_string, file_path)                          logger.info(f\"Data ingestion complete. {records_loaded} records processed.\")             return records_loaded         else:             logger.warning(\"No valid records to load after validation and deduplication.\")             return 0              except Exception as e:         logger.error(f\"Error in data ingestion process: {str(e)}\")         raise   I encapsulated the cleaning operations in a dedicated class with specialized methods for each type of cleaning:  class CleanData:     \"\"\"     A class for handling different types of data cleaning operations.     Each method handles a specific type of cleaning.     \"\"\"          @staticmethod     def handle_missing_values(df):         \"\"\"Handle missing values in the DataFrame.\"\"\"         logger.info(\"Cleaning: Handling missing values...\")         df_cleaned = df.copy()         df_cleaned['Location'] = df_cleaned['Location'].fillna('Unknown')         return df_cleaned          @staticmethod     def clean_price_values(df):         \"\"\"Clean price values by removing currency symbols.\"\"\"         logger.info(\"Cleaning: Cleaning price values...\")         df_cleaned = df.copy()                  # Handle price with currency notation         df_cleaned.loc[df_cleaned['Price'].str.contains('USD', na=False), 'Price'] = \\             df_cleaned.loc[df_cleaned['Price'].str.contains('USD', na=False), 'Price'].str.replace('USD', '')                  # Strip whitespace         df_cleaned['Price'] = df_cleaned['Price'].str.strip()                  return df_cleaned          # More cleaning methods...          @classmethod     def apply_all_cleaners(cls, df):         \"\"\"Apply all cleaning methods in sequence.\"\"\"         logger.info(\"Starting comprehensive data cleaning...\")                  df_result = df.copy()         df_result = cls.handle_missing_values(df_result)         df_result = cls.standardize_data_types(df_result)         df_result = cls.remove_whitespace_values(df_result)         df_result = cls.clean_price_values(df_result)         df_result = cls.clean_date_values(df_result)                  logger.info(f\"Comprehensive data cleaning complete. Processed {len(df_result)} rows.\")         return df_result   2. Data Transformation Layer  After landing raw data, the transformation component converts it into a proper dimensional model:  def process_sales_data(engine):     \"\"\"     Process and transform sales data from raw to fact table.     \"\"\"          try:         # Get dimension lookups         channel_ids = populate_dim_channel(engine)         location_ids = populate_dim_location(engine)         product_ids = populate_dim_product(engine)         retailer_ids = populate_dim_retailer(engine)         date_ids = populate_dim_date(engine)                  # Query to get raw sales data         query = \"\"\"         SELECT \"SaleID\", \"ProductID\", \"RetailerID\", \"Channel\", \"Location\",                 \"Quantity\", \"Price\", \"Date\"         FROM raw.sales         WHERE \"SaleID\" NOT IN (             SELECT sale_id::VARCHAR FROM transformed.fact_sales         )         ORDER BY \"SaleID\" ASC         \"\"\"                  with engine.begin() as conn:             # Count total records to process             count_query = \"\"\"             SELECT COUNT(*) FROM raw.sales             WHERE \"SaleID\" NOT IN (                 SELECT sale_id::VARCHAR FROM transformed.fact_sales             )             \"\"\"             result = conn.execute(text(count_query))             total_records = result.fetchone()[0]             logger.info(f\"Found {total_records} new sales records to process\")                          if total_records == 0:                 logger.info(\"No new records to process\")                 return 0                          result = conn.execute(text(query))             sales = [dict(zip(result.keys(), row)) for row in result]                          # Transform data             logger.info(f\"Processing {len(sales)} sales records\")             processed_count = 0             fact_records = []                          # Process each sale record             for sale in sales:                 try:                     # Data transformations here...                                          # Create fact record                     fact_record = {                         \"sale_id\": sale_id,                         \"product_id\": product_id,                         \"retailer_id\": retailer_id,                         \"location_id\": location_id,                         \"channel_id\": channel_id,                         \"date_id\": date_id,                         \"quantity\": quantity,                         \"unit_price\": unit_price,                         \"total_amount\": total_amount                     }                     fact_records.append(fact_record)                 except Exception as e:                     logger.error(f\"Error processing sale {sale['SaleID']}: {str(e)}\")                          # Insert fact records             if fact_records:                 try:                     insert_query = \"\"\"                     INSERT INTO transformed.fact_sales (                         sale_id, product_id, retailer_id, location_id,                          channel_id, date_id, quantity, unit_price, total_amount                     )                     VALUES (                         :sale_id, :product_id, :retailer_id, :location_id,                          :channel_id, :date_id, :quantity, :unit_price, :total_amount                     )                     ON CONFLICT (sale_id) DO NOTHING                     \"\"\"                     # Use a new transaction to ensure atomicity                     with engine.begin() as insert_conn:                         insert_conn.execute(text(insert_query), fact_records)                                          processed_count = len(fact_records)                     logger.info(f\"Inserted {len(fact_records)} records into fact_sales\")                 except Exception as e:                     logger.error(f\"Error inserting: {str(e)}\")                          logger.info(f\"Successfully processed {processed_count} sales records\")             return processed_count     except Exception as e:         logger.error(f\"Error processing sales data: {str(e)}\")         raise   3. dbt Transformation Models  For the analytical layer, I implemented dbt models that further refine the data:  First, a staging model to standardize raw data formats:  -- stg_sales.sql  with source as (     select * from {{ source('postgres', 'sales') }} ),  cleaned as (     select         \"SaleID\"::integer as sale_id,         nullif(\"ProductID\", '')::integer as product_id,         \"ProductName\" as product_name,         \"Brand\" as brand,         \"Category\" as category,         \"RetailerID\"::integer as retailer_id,         \"RetailerName\" as retailer_name,         \"Channel\" as channel,         coalesce(nullif(\"Location\", ''), 'Unknown') as location,         case              when \"Quantity\" ~ '^-?\\d+$' then \"Quantity\"::integer             else null         end as quantity,         case             when \"Price\" ~ '^\\d+$' then \"Price\"::decimal             when \"Price\" ~ '^\\d+USD$' then replace(\"Price\", 'USD', '')::decimal             else null         end as price,         case             when \"Date\" ~ '^\\d{4}-\\d{2}-\\d{2}$' then \"Date\"::date             when \"Date\" ~ '^\\d{4}/\\d{2}/\\d{2}$' then to_date(\"Date\", 'YYYY/MM/DD')             else null         end as date,         batch_id,         source_file,         inserted_at     from source ),  final as (     select         sale_id,         product_id,         product_name,         brand,         category,         retailer_id,         retailer_name,         channel,         location,         case when quantity &lt;= 0 then null else quantity end as quantity,         price,         date,         batch_id,         source_file,         inserted_at,         current_timestamp as transformed_at     from cleaned     where          sale_id is not null         and product_id is not null         and retailer_id is not null         and date is not null         and quantity is not null         and price is not null )  select * from final   Then dimensional models built on top of staging:  -- fact_sales.sql  {{   config(     unique_key = 'sale_id',     indexes = [       {'columns': ['sale_id'], 'unique': True},       {'columns': ['product_id']},       {'columns': ['retailer_id']},       {'columns': ['location_id']},       {'columns': ['channel_id']},       {'columns': ['date_id']}     ]   ) }}  with stg_sales as (     select * from {{ ref('stg_sales') }} ),  dim_product as (     select * from {{ ref('dim_product') }} ),  dim_location as (     select * from {{ ref('dim_location') }} ),  -- Create dimension references for retailer and channel dim_retailer as (     select distinct         retailer_id,         retailer_name     from stg_sales ),  dim_channel as (     select         channel,         {{ dbt_utils.generate_surrogate_key(['channel']) }} as channel_id     from stg_sales     group by channel ),  -- Final fact sales table final as (     select         s.sale_id,         s.product_id,         s.retailer_id,         l.location_id,         c.channel_id,         s.date as date_id,         s.quantity,         s.price / nullif(s.quantity, 0)::numeric(10, 2) as unit_price,         s.price::numeric(12, 2) as total_amount,         s.transformed_at     from stg_sales s     inner join dim_location l on l.location = s.location     inner join dim_channel c on c.channel = s.channel     {% if is_incremental() %}     where s.transformed_at &gt; (select max(transformed_at) from {{ this }})     {% endif %} )  select * from final    4. Orchestration with Airflow  The Airflow DAGs orchestrate the entire pipeline. I implemented two primary DAGs:     The Sales Data Pipeline for ingestion and initial transformation:   # Define task dependencies check_file_exists &gt;&gt; check_and_ingest_data &gt;&gt; transform_raw_data &gt;&gt; archive_file      The dbt Transformation Pipeline for analytical models:   # Define dbt commands dbt_deps_cmd = f\"\"\" cd {DBT_PROJECT_DIR} &amp;&amp;  dbt deps --profiles-dir {DBT_PROFILES_DIR} --target {DBT_TARGET} \"\"\"  dbt_run_staging_cmd = f\"\"\" cd {DBT_PROJECT_DIR} &amp;&amp;  dbt run --models \"staging.*\" --profiles-dir {DBT_PROFILES_DIR} --target {DBT_TARGET} \"\"\"  dbt_run_marts_cmd = f\"\"\" cd {DBT_PROJECT_DIR} &amp;&amp;  dbt run --models \"marts.*\" --profiles-dir {DBT_PROFILES_DIR} --target {DBT_TARGET} \"\"\"  dbt_test_cmd = f\"\"\" cd {DBT_PROJECT_DIR} &amp;&amp;  dbt test --profiles-dir {DBT_PROFILES_DIR} --target {DBT_TARGET} \"\"\"  # Define task dependencies check_and_ingest_data &gt;&gt; install_dependencies &gt;&gt; run_staging_models &gt;&gt; run_mart_models &gt;&gt; test_models   Advanced Features  Self-healing Data Flow  A key feature of this pipeline is its â€œself-healingâ€ capability. Both DAGs automatically check if the required data exists before proceeding, and trigger upstream processes if needed:  class RawDataSensor(BaseSensorOperator):     \"\"\"     Sensor to check if there's data in the raw.sales table.     \"\"\"     @apply_defaults     def __init__(self, conn_id=\"sales_db\", *args, **kwargs):         super().__init__(*args, **kwargs)         self.conn_id = conn_id      def poke(self, context):         hook = PostgresHook(postgres_conn_id=self.conn_id)         sql = \"SELECT COUNT(*) FROM raw.sales\"         count = hook.get_first(sql)[0]         self.log.info(f\"Found {count} rows in raw.sales table\")         return count &gt; 0   def check_and_ingest_data(csv_file_path, conn_id=\"sales_db\", **context):     \"\"\"     Check if data exists in raw.sales and ingest if empty.     \"\"\"     hook = PostgresHook(postgres_conn_id=conn_id)          # Check if data exists     sql = \"SELECT COUNT(*) FROM raw.sales\"     count = hook.get_first(sql)[0]          # If data exists, return True     if count &gt; 0:         context['ti'].xcom_push(key='data_already_exists', value=True)         return True          # If no data, perform ingestion     try:         ingest_main = import_ingest_module()         records_loaded = ingest_main(csv_file_path)                  # Verify ingestion was successful         if records_loaded &gt; 0:             context['ti'].xcom_push(key='records_loaded', value=records_loaded)             return True         else:             context['ti'].xcom_push(key='ingest_failed', value=True)             return False     except Exception as e:         context['ti'].xcom_push(key='ingest_error', value=str(e))         raise   This design enables either pipeline to be triggered independently without failures, creating a more resilient system.  Data Quality Testing  Comprehensive data quality checks are implemented in both Python and dbt:     Python validation for source data:   def validate_data(df):     \"\"\"     Validate data quality and identify invalid records.     Returns a boolean indicating if the data is valid and a list of invalid indices.     \"\"\"     logger.info(f\"Validating data... Total records: {len(df)}\")          # Track invalid rows for logging     invalid_rows = {         'dates': [],         'quantities': [],         'prices': [],         'all': set()  # Use a set to avoid duplicates     }          # Check for invalid dates     for idx, date_str in enumerate(df['Date']):         try:             # Try to parse the date             if isinstance(date_str, str) and date_str:                 # Handle different date formats                 if '/' in date_str:                     datetime.strptime(date_str, '%Y/%m/%d')                 else:                     datetime.strptime(date_str, '%Y-%m-%d')             else:                 # Empty or non-string date                 invalid_rows['dates'].append((idx, date_str))                 invalid_rows['all'].add(idx)         except ValueError:             invalid_rows['dates'].append((idx, date_str))             invalid_rows['all'].add(idx)          # More validation checks...          return is_valid, list(invalid_rows['all'])      dbt tests for transformed data:   # schema.yml for fact_sales version: 2  models:   - name: fact_sales     description: \"Fact table for sales with related dimension keys\"     columns:       - name: sale_id         description: \"The primary key for the sales transaction\"         tests:           - unique           - not_null              - name: product_id         description: \"Foreign key to product dimension\"         tests:           - not_null           - relationships:               to: ref('dim_product')               field: product_id              # Additional column tests...              - name: quantity         description: \"Number of items sold\"         tests:           - not_null           - positive_values   Incremental Processing  The pipeline implements incremental processing at multiple levels:     Python-based ETL uses ID-based tracking:   # Query to get only new records  query = \"\"\" SELECT \"SaleID\", \"ProductID\", \"RetailerID\", \"Channel\", \"Location\",         \"Quantity\", \"Price\", \"Date\" FROM raw.sales WHERE \"SaleID\" NOT IN (     SELECT sale_id::VARCHAR FROM transformed.fact_sales ) ORDER BY \"SaleID\" ASC \"\"\"      dbt models use incremental materialization:    {% if is_incremental() %} where s.transformed_at &gt; (select max(transformed_at) from {{ this }}) {% endif %}    This two-tiered approach ensures efficient processing of only new or changed data.  Containerization and Deployment  The entire solution is containerized using Docker for consistent deployment:  services:   postgres:     image: postgres:latest     environment:       - POSTGRES_USER=postgres       - POSTGRES_PASSWORD=mysecretpassword       - POSTGRES_MULTIPLE_DATABASES=airflow,sales     volumes:       - ./initdb:/docker-entrypoint-initdb.d       - postgres-db-volume:/var/lib/postgresql/data     healthcheck:       test: [\"CMD\", \"pg_isready\", \"-U\", \"postgres\"]       interval: 5s       retries: 5     ports:       - \"5433:5432\"     restart: always    airflow-webserver:     &lt;&lt;: *airflow-common     command: webserver     ports:       - 8081:8080     healthcheck:       test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8081/health\"]       interval: 10s       timeout: 10s       retries: 5     restart: always    # Additional services...   Scalability Approaches  As the data volume grows, several scalability enhancements can be implemented:  1. Partitioning for Larger Datasets  For larger datasets, implementing table partitioning in PostgreSQL can significantly improve performance:  -- Example of adding partitioning to fact_sales CREATE TABLE analytics.fact_sales (     sale_id INTEGER,     -- other columns...     date_id DATE NOT NULL ) PARTITION BY RANGE (date_id);  -- Create partitions by month CREATE TABLE fact_sales_2024_q1 PARTITION OF fact_sales     FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');  CREATE TABLE fact_sales_2024_q2 PARTITION OF fact_sales     FOR VALUES FROM ('2024-04-01') TO ('2024-07-01');   2. Parallel Processing with Airflow  For processing large volumes of data, implementing parallel task execution in Airflow:  # Create parallel tasks for processing different data segments for segment in get_data_segments():     process_segment = PythonOperator(         task_id=f'process_segment_{segment}',         python_callable=process_data_segment,         op_kwargs={'segment': segment},         dag=dag,     )          # Set dependencies     check_and_ingest_data &gt;&gt; process_segment &gt;&gt; merge_segments   3. Enhanced Incremental Processing  The current incremental approach can be enhanced with timestamp-based windowing:  def extract_incremental_data(start_time, end_time, batch_size=10000):     \"\"\"Extract data in time-bounded batches for efficient processing.\"\"\"     current_position = start_time          while current_position &lt; end_time:         next_position = min(current_position + timedelta(hours=1), end_time)                  query = \"\"\"         SELECT * FROM raw.sales         WHERE inserted_at &gt;= %s AND inserted_at &lt; %s         ORDER BY inserted_at         LIMIT %s         \"\"\"                  yield execute_query(query, (current_position, next_position, batch_size))         current_position = next_position   This approach reduces memory pressure when dealing with large datasets.  Optimization Techniques  1. Database Indexing Strategy  Carefully designed indexes dramatically improve query performance:  -- Indexes for the fact table CREATE INDEX idx_fact_sales_product_id ON transformed.fact_sales(product_id); CREATE INDEX idx_fact_sales_retailer_id ON transformed.fact_sales(retailer_id); CREATE INDEX idx_fact_sales_date_id ON transformed.fact_sales(date_id); CREATE INDEX idx_fact_sales_channel_id ON transformed.fact_sales(channel_id);   2. Memory-Efficient Processing  For large datasets, implement batch processing to control memory usage:  def process_large_dataset(file_path, batch_size=10000):     \"\"\"Process large CSV files in batches to control memory usage.\"\"\"     # Use pandas chunking for memory efficiency     for chunk in pd.read_csv(file_path, chunksize=batch_size):         # Validate and clean the chunk         is_valid, invalid_indices = validate_data(chunk)         if not is_valid:             chunk = CleanData.apply_all_cleaners(chunk)             chunk = filter_invalid_records(chunk, invalid_indices)                      # Process the cleaned chunk         load_to_raw(chunk, get_db_connection_string(), file_path)   3. Airflow Task Configuration  Optimizing Airflow task configuration for better resource utilization:  # Task configuration for better resource management task = PythonOperator(     task_id='transform_raw_data',     python_callable=transform_main,     executor_config={         'cpu_millicores': 1000,         'memory_mb': 2048,     },     dag=dag, )   Conclusion  This data pipeline demonstrates how modern tools and architectural patterns can create a robust, production-ready data infrastructure. By combining Airflowâ€™s orchestration capabilities with dbtâ€™s transformation power and a well-designed schema, weâ€™ve built a system that can handle real-world data challenges while maintaining flexibility for future growth.  Key takeaways from this implementation:     The value of a layered architectural approach (medallion pattern)   The importance of separating validation from cleaning for better data quality management   The benefits of self-healing data flows that can recover from failures   How containerization provides environment consistency across development and production   While no data pipeline is ever truly â€œcompleteâ€ (data requirements evolve continuously), this implementation provides a solid foundation that can adapt to changing business needs. The patterns and practices demonstrated here can help create more resilient, maintainable data systems for organizations of any size.  The complete code for this project is available on GitHub at samuelTyh/airflow-dbt-sales-analytics.  Future Work  Looking ahead, this pipeline could be enhanced with:     Real-time streaming capabilities: Integrating a streaming solution like Kafka for near real-time data processing   Advanced data quality monitoring: Adding automated data quality monitoring with Great Expectations or dbt expectations   ML feature engineering: Extending the pipeline to generate features for machine learning models   Cloud-native deployment: Adapting the architecture for cloud platforms with services like AWS Glue, Azure Data Factory, or Google Cloud Dataflow   These enhancements would further extend the capabilities of the pipeline while maintaining the core architectural principles that make it reliable and maintainable.  What data pipeline patterns have you found most effective in your work? Share your thoughts and questions in the comments below!"
  },
  
  {
    "title": "Building a Scalable ETL Pipeline for AdTech Analytics",
    "url": "/posts/building-a-scalable-etl-pipeline-for-adtech-analytics/",
    "categories": "Data Engineering",
    "tags": "python, postgres, etl, clickhouse",
    "date": "2025-04-21 17:47:00 +0200",
    "content": "In the world of digital advertising, data is everything. Transforming raw operational data into actionable insights requires robust analytics pipelines. Recently, I implemented a comprehensive ETL (Extract, Transform, Load) solution for an advertising platform that moves data from PostgreSQL to ClickHouse for high-performance analytics. Let me walk you through the process, design decisions, and implementation details.  Overview of the Challenge  The challenge was to build a pipeline that would:     Extract campaign, impression, and click data from a PostgreSQL operational database   Transform the data into an analytics-friendly format   Load it into ClickHouse, a columnar DBMS optimized for analytical queries   Create materialized views for key advertising KPIs   The solution needed to handle both initial data loads and incremental updates, with robust error handling and monitoring capabilities.  Architecture Design  After reviewing the requirements, I designed the following architecture:  PostgreSQL (Source) â†’ ETL Pipeline â†’ ClickHouse (Target)   Source Database (PostgreSQL)  The operational database contained four primary tables:    advertiser: Information about companies running ad campaigns   campaign: Campaign configurations with bid amounts and budgets   impressions: Records of ads being displayed   clicks: Records of users clicking on ads   Target Schema (ClickHouse)  For the analytical layer, I designed a dimensional model with:     Dimension tables:            dim_advertiser       dim_campaign           Fact tables:            fact_impressions       fact_clicks           Materialized views for KPIs:            Campaign CTR (Click-Through Rate)       Daily performance metrics       Campaign daily performance       Cost efficiency metrics       Advertiser performance overviews           Implementation Details  1. Setting Up the Core Components  I implemented the ETL pipeline in Python 3.12 with a modular design to separate concerns:  from .config import AppConfig, PostgresConfig, ClickhouseConfig, ETLConfig from .db import PostgresConnector, ClickhouseConnector from .schema import SchemaManager from .pipeline import (     DataExtractor, DataTransformer, DataLoader, ETLPipeline )   2. Database Connectors  The first components I built were the database connectors, which encapsulate connection management and query execution:  class PostgresConnector:     \"\"\"PostgreSQL connection manager.\"\"\"          def __init__(self, config: PostgresConfig):         \"\"\"Initialize with PostgreSQL configuration.\"\"\"         self.config = config         self.conn = None          def connect(self) -&gt; bool:         \"\"\"Establish connection to PostgreSQL database.\"\"\"         try:             self.conn = psycopg.connect(                 self.config.connection_string,                 autocommit=False,             )             logger.info(\"Connected to PostgreSQL database\")             return True         except Exception as e:             logger.error(f\"Failed to connect to PostgreSQL: {e}\")             return False          # Additional methods for query execution, etc.   Similarly, I implemented a ClickhouseConnector for managing ClickHouse connections:  class ClickhouseConnector:     \"\"\"ClickHouse connection manager.\"\"\"          def __init__(self, config: ClickhouseConfig):         \"\"\"Initialize with ClickHouse configuration.\"\"\"         self.config = config         self.client = None          # Methods for connection, query execution, etc.   3. Schema Management  I created a SchemaManager class to handle ClickHouse schema setup and updates:  class SchemaManager:     \"\"\"Manages ClickHouse schema creation and updates.\"\"\"          def __init__(self, db_connector: ClickhouseConnector, config: ETLConfig):         \"\"\"Initialize with ClickHouse connector and configuration.\"\"\"         self.db = db_connector         self.config = config          def setup_schema(self) -&gt; bool:         \"\"\"Initialize ClickHouse schema if not exists.\"\"\"         try:             self.db.execute_file(self.config.schema_path)             logger.info(\"ClickHouse schema initialized successfully\")             return True         except Exception as e:             logger.error(f\"Error setting up ClickHouse schema: {e}\")             return False   4. The ETL Pipeline Components  The core of the implementation consists of three main components:  A. Data Extractor  class DataExtractor:     \"\"\"Extracts data from PostgreSQL source database.\"\"\"          def __init__(self, db: PostgresConnector):         \"\"\"Initialize with PostgreSQL connector.\"\"\"         self.db = db          def extract_advertisers(self, since: datetime) -&gt; List[Tuple]:         \"\"\"Extract advertisers updated since the given timestamp.\"\"\"         query = \"\"\"             SELECT id, name, updated_at, created_at              FROM advertiser             WHERE updated_at &gt; %s OR created_at &gt; %s         \"\"\"         return self.db.execute_query(query, (since, since))          # Additional methods for extracting campaigns, impressions, clicks   B. Data Transformer  class DataTransformer:     \"\"\"Transforms data for loading into ClickHouse.\"\"\"          @staticmethod     def transform_advertisers(rows: List[Tuple]) -&gt; List[Tuple]:         \"\"\"Transform advertiser data for ClickHouse.\"\"\"         transformed = []         for adv_id, name, updated_at, created_at in rows:             transformed.append((                 adv_id,                 name,                 updated_at or datetime.now(),                 created_at or datetime.now()             ))         return transformed          # Additional transformation methods   C. Data Loader  class DataLoader:     \"\"\"Loads transformed data into ClickHouse.\"\"\"          def __init__(self, db: ClickhouseConnector):         \"\"\"Initialize with ClickHouse connector.\"\"\"         self.db = db          def load_advertisers(self, data: List[Tuple]) -&gt; int:         \"\"\"Load advertiser data into ClickHouse.\"\"\"         if not data:             return 0                      query = \"\"\"             INSERT INTO analytics.dim_advertiser             (advertiser_id, name, updated_at, created_at)             VALUES         \"\"\"         self.db.execute_query(query, data)         return len(data)          # Additional loading methods   5. Orchestrating the ETL Process  I created an ETLPipeline class to orchestrate the entire process:  class ETLPipeline:     \"\"\"Main ETL pipeline that orchestrates extract, transform, and load.\"\"\"          def __init__(         self,          extractor: DataExtractor,          transformer: DataTransformer,          loader: DataLoader     ):         \"\"\"Initialize with extractor, transformer, and loader components.\"\"\"         self.extractor = extractor         self.transformer = transformer         self.loader = loader         self.last_sync = {             'advertiser': datetime.min,             'campaign': datetime.min,             'impressions': datetime.min,             'clicks': datetime.min         }                  # Tracking for sync statistics         self.sync_stats = {             'advertiser': 0,             'campaign': 0,             'impressions': 0,             'clicks': 0         }          def run_sync_cycle(self) -&gt; bool:         \"\"\"Run a complete ETL cycle.\"\"\"         try:             logger.info(\"Starting ETL sync cycle\")                          # Reset sync statistics             for key in self.sync_stats:                 self.sync_stats[key] = 0                          # Sync dimension tables first             self.sync_stats['advertiser'] = self.sync_advertisers()             self.sync_stats['campaign'] = self.sync_campaigns()                          # Then sync fact tables             self.sync_stats['impressions'] = self.sync_impressions()              self.sync_stats['clicks'] = self.sync_clicks()                          # Log sync summary             logger.info(\"ETL sync cycle completed successfully\")             logger.info(f\"Sync summary: \"                        f\"Advertisers: {self.sync_stats['advertiser']}, \"                        f\"Campaigns: {self.sync_stats['campaign']}, \"                        f\"Impressions: {self.sync_stats['impressions']}, \"                        f\"Clicks: {self.sync_stats['clicks']}\")                          return True                      except Exception as e:             logger.error(f\"ETL sync cycle failed: {e}\")             return False   6. Implementing Incremental Updates  One of the most critical aspects of the implementation was handling incremental updates efficiently. I designed the system to track the last sync timestamp for each entity:  def sync_advertisers(self) -&gt; int:     \"\"\"Sync advertisers from PostgreSQL to ClickHouse.\"\"\"     try:         # Extract only data updated since last sync         rows = self.extractor.extract_advertisers(self.last_sync['advertiser'])         if not rows:             logger.info(\"No new advertisers to sync\")             return 0                  # Transform and load         data = self.transformer.transform_advertisers(rows)         count = self.loader.load_advertisers(data)                  # Update the last sync timestamp         latest_update = self.last_sync['advertiser']         for _, _, updated_at, created_at in rows:             if updated_at and updated_at &gt; latest_update:                 latest_update = updated_at             if created_at and created_at &gt; latest_update:                 latest_update = created_at                  self.last_sync['advertiser'] = latest_update         logger.info(f\"Synced {count} advertisers\")         return count              except Exception as e:         logger.error(f\"Error syncing advertisers: {e}\")         return 0   7. Main Service Loop  Finally, I implemented a main service class to tie everything together:  def run_service(self, run_once: bool = False, interval: Optional[int] = None, force_full_sync: bool = False) -&gt; None:     \"\"\"Run the ETL service continuously or once.\"\"\"     sync_interval = interval or self.config.etl.sync_interval          if not self.initialize():         self.logger.error(\"Service initialization failed. Exiting.\")         sys.exit(1)          if force_full_sync:         self.logger.info(\"Forcing full sync - resetting all sync timestamps\")         for key in self.etl_pipeline.last_sync:             self.etl_pipeline.last_sync[key] = datetime.min          self.logger.info(f\"AdTech ETL service started with sync interval: {sync_interval}s\")          try:         if run_once:             success = self.run_sync()             if not success:                 sys.exit(1)         else:             # Continuous operation             while True:                 success = self.run_sync()                 if not success:                     self.logger.warning(f\"Waiting {sync_interval} seconds before retry...\")                                  self.logger.info(f\"Sleeping for {sync_interval} seconds...\")                 time.sleep(sync_interval)                      except KeyboardInterrupt:         self.logger.info(\"ETL service interrupted, shutting down\")     except Exception as e:         self.logger.critical(f\"Unexpected error: {e}\")         sys.exit(1)     finally:         # Clean up resources         if hasattr(self, 'pg_connector'):             self.pg_connector.close()         if hasattr(self, 'ch_connector'):             self.ch_connector.close()   ClickHouse Optimization  A key part of the solution was optimizing the ClickHouse schema for analytical queries:  -- Dimension tables with ReplacingMergeTree engine CREATE TABLE IF NOT EXISTS analytics.dim_advertiser (     advertiser_id UInt32,     name String,     updated_at DateTime,     created_at DateTime ) ENGINE = ReplacingMergeTree(updated_at) ORDER BY (advertiser_id);  -- Fact tables with partitioning CREATE TABLE IF NOT EXISTS analytics.fact_impressions (     impression_id UInt32,     campaign_id UInt32,     event_date Date,     event_time DateTime,     created_at DateTime ) ENGINE = MergeTree() PARTITION BY toYYYYMM(event_date) ORDER BY (campaign_id, event_date);   Materialized Views for KPIs  I created several materialized views to pre-calculate common KPIs:  -- Materialized view for campaign CTR CREATE MATERIALIZED VIEW IF NOT EXISTS analytics.mv_campaign_ctr (     campaign_id UInt32,     campaign_name String,     advertiser_name String,     impressions UInt64,     clicks UInt64,     ctr Float64 ) ENGINE = SummingMergeTree() ORDER BY (campaign_id) POPULATE AS SELECT     c.campaign_id,     c.name AS campaign_name,     a.name AS advertiser_name,     COUNT(DISTINCT i.impression_id) AS impressions,     COUNT(DISTINCT cl.click_id) AS clicks,     COUNT(DISTINCT cl.click_id) / COUNT(DISTINCT i.impression_id) AS ctr FROM dim_campaign c JOIN dim_advertiser a ON c.advertiser_id = a.advertiser_id LEFT JOIN fact_impressions i ON c.campaign_id = i.campaign_id LEFT JOIN fact_clicks cl ON c.campaign_id = cl.campaign_id GROUP BY c.campaign_id, c.name, a.name;   Testing  I implemented comprehensive testing with pytest to ensure the reliability of the ETL pipeline:     Unit tests for individual components   Integration tests for the end-to-end pipeline   Schema tests for database schema validation   For example, the unit tests for the Transformer component:  @pytest.mark.unit class TestDataTransformer:     \"\"\"Tests for DataTransformer.\"\"\"      def test_transform_advertisers(self):         \"\"\"Test transforming advertiser data.\"\"\"         now = datetime.now()         input_data = [(1, 'Advertiser A', now, now)]                  transformer = DataTransformer()         result = transformer.transform_advertisers(input_data)                  assert result == [(1, 'Advertiser A', now, now)]   Deployment with Docker  I containerized the entire solution using Docker to ensure consistent operation:  FROM python:3.12-slim  WORKDIR /app  # Install dependencies COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt  # Copy ETL files COPY . .  # Run the ETL script CMD [\"python\", \"main.py\"]   And orchestrated the services with Docker Compose:  services:   # PostgreSQL   postgres:     image: postgres:17     container_name: psql_source     env_file: .env     ports:       - \"6543:5432\"     volumes:       - postgres_data:/var/lib/postgresql/data        # ClickHouse   clickhouse:     image: clickhouse/clickhouse-server     container_name: ch_analytics     env_file: .env     ports:       - \"8124:8123\"       - \"9001:9000\"     volumes:       - clickhouse_data:/var/lib/clickhouse      # ETL Service   etl:     build:       context: ./etl       dockerfile: Dockerfile.etl     container_name: adtech_etl     restart: unless-stopped     depends_on:       postgres:         condition: service_healthy       clickhouse:         condition: service_healthy     env_file: .env     volumes:       - ./etl:/app   CI/CD Pipeline  To ensure code quality, I set up a GitHub Actions workflow:  name: CI  on:   push:     branches: [ main, dev ]   pull_request:     branches: [ main, dev ]  jobs:   lint:     name: Code Quality Checks     runs-on: ubuntu-latest     steps:       - uses: actions/checkout@v4              - name: Set up Python 3.12         uses: actions/setup-python@v5         with:           python-version: '3.12'              - name: Install dependencies         run: |           python -m pip install --upgrade pip           pip install ruff==0.11.0              - name: Lint with ruff         run: python -m ruff check etl tests    test:     name: Unit Tests     runs-on: ubuntu-latest     steps:       - uses: actions/checkout@v4              - name: Set up Python 3.12         uses: actions/setup-python@v5         with:           python-version: '3.12'              - name: Install dependencies         run: |           python -m pip install --upgrade pip           pip install -r etl/requirements.txt           pip install pytest==8.3.5              - name: Run unit tests         run: python -m pytest -xvs -m \"unit\"   Scalability Approaches  As data volumes grow in advertising platforms, the ETL pipeline must scale accordingly. Here are the key scalability approaches I implemented and recommend for further expansion:  1. Horizontal Scaling with Distributed Processing  The current architecture can be enhanced for horizontal scalability by implementing:  class DistributedETLPipeline(ETLPipeline):     \"\"\"Distributed version of the ETL pipeline that supports partitioned processing.\"\"\"          def __init__(self, extractor, transformer, loader, partition_count=4):         super().__init__(extractor, transformer, loader)         self.partition_count = partition_count              def partition_data(self, table_name, date_field, partition_key):         \"\"\"Create logical partitions for parallel processing.\"\"\"         partition_ranges = []         # Calculate partition boundaries based on time or ID ranges         return partition_ranges              def run_partitioned_sync(self, table_name, executor):         \"\"\"Execute sync operations across multiple partitions in parallel.\"\"\"         partitions = self.partition_data(table_name, 'created_at', 'id')         futures = []                  for partition in partitions:             # Submit each partition for parallel execution             future = executor.submit(self.sync_partition, table_name, partition)             futures.append(future)                      # Gather results         results = [future.result() for future in futures]         return sum(results)   With this approach, we can use Pythonâ€™s concurrent.futures or distributed task queues like Celery to process partitions in parallel across multiple worker nodes.  2. Time-Based Batching for High-Volume Tables  For fact tables with millions of daily records (impressions, clicks), implementing time-based batching reduces memory pressure:  def sync_impressions_with_batching(self, batch_size=10000, time_window_hours=1) -&gt; int:     \"\"\"Sync impressions with time-based batching.\"\"\"     total_synced = 0     current_time = self.last_sync['impressions']     end_time = datetime.now()          while current_time &lt; end_time:         # Calculate next batch window         next_window = current_time + timedelta(hours=time_window_hours)         if next_window &gt; end_time:             next_window = end_time                      # Extract and process just this time window         rows = self.extractor.extract_impressions_by_window(current_time, next_window)         if rows:             data = self.transformer.transform_impressions(rows)             count = self.loader.load_impressions(data)             total_synced += count                      # Move to next window         current_time = next_window              # Update the final sync timestamp     self.last_sync['impressions'] = end_time     return total_synced   3. Database-Level Scaling  As the system scales, the database architecture can be enhanced:  PostgreSQL Scaling:    Implement read replicas to isolate the ETL read load from operational writes   Use logical replication with PostgreSQLâ€™s Change Data Capture (CDC) features   Consider a replication-based CDC tool like Debezium for near real-time data streaming   ClickHouse Scaling:    Implement a ClickHouse cluster with data sharding across multiple nodes   Optimize the sharding key for commonly queried dimensions (e.g., by campaign_id)   Implement distributed tables to abstract the sharding complexity:   -- Distributed table definition CREATE TABLE IF NOT EXISTS analytics.dist_fact_impressions  AS analytics.fact_impressions ENGINE = Distributed(cluster_name, analytics, fact_impressions, rand());   4. Resilient Work Queue Architecture  For extreme scale, transition from a scheduled polling approach to an event-driven architecture:  # In a message consumer service def process_etl_message(self, message):     \"\"\"Process a message from the ETL work queue.\"\"\"     try:         entity_type = message['entity_type']         batch_id = message['batch_id']         time_range = message.get('time_range', None)                  # Process the specific batch         if entity_type == 'impressions':             self.etl_pipeline.sync_impressions_batch(batch_id, time_range)         elif entity_type == 'clicks':             self.etl_pipeline.sync_clicks_batch(batch_id, time_range)         # etc.                  # Acknowledge successful processing         self.queue.acknowledge(message['id'])     except Exception as e:         # Failed processing - handle with retry logic         self.queue.retry(message['id'])         logger.error(f\"Failed to process ETL message: {e}\")   This approach works well with Apache Kafka, RabbitMQ, or cloud-native solutions like AWS SQS/SNS for truly decoupled processing.  Optimization Techniques  Beyond the initial implementation, Iâ€™ve identified several optimization opportunities:  1. Query Optimization for Extraction  Improving extraction performance through optimized queries:  def extract_impressions_optimized(self, since: datetime) -&gt; List[Tuple]:     \"\"\"Extract impressions with optimized query performance.\"\"\"     query = \"\"\"         SELECT id, campaign_id, created_at         FROM impressions         WHERE created_at &gt; %s         ORDER BY created_at         LIMIT 50000  -- Batch size control     \"\"\"     return self.db.execute_query(query, (since,))   Additionally, I recommend adding appropriate indexes to source tables:  -- Add index for incremental extraction performance CREATE INDEX IF NOT EXISTS idx_impressions_created_at ON impressions(created_at); CREATE INDEX IF NOT EXISTS idx_clicks_created_at ON clicks(created_at);   2. Batch Processing and Bulk Loading  Implementing bulk loading operations for ClickHouse significantly improves throughput:  def load_impressions_bulk(self, data: List[Tuple]) -&gt; int:     \"\"\"Load impression data into ClickHouse using efficient bulk loading.\"\"\"     if not data:         return 0          # Prepare data for bulk insert     formatted_data = []     for imp_id, campaign_id, event_date, event_time, created_at in data:         formatted_data.append({             'impression_id': imp_id,             'campaign_id': campaign_id,             'event_date': event_date,             'event_time': event_time,             'created_at': created_at         })          # Execute bulk insert     self.db.client.execute(         \"INSERT INTO analytics.fact_impressions VALUES\",         formatted_data     )     return len(data)   3. Memory Management  For large datasets, implement iterator-based processing to avoid loading entire result sets into memory:  def extract_large_dataset(self, since: datetime, batch_size=10000):     \"\"\"Extract large datasets using server-side cursors to control memory usage.\"\"\"     query = \"\"\"         SELECT id, campaign_id, created_at          FROM impressions         WHERE created_at &gt; %s         ORDER BY id     \"\"\"          with self.db.conn.cursor(name='large_extract_cursor') as cursor:         cursor.execute(query, (since,))                  while True:             batch = cursor.fetchmany(batch_size)             if not batch:                 break             yield batch   4. Compression and Data Type Optimizations  Optimize ClickHouse storage by carefully selecting compression and data types:  -- Optimized fact table with compression and efficient data types CREATE TABLE IF NOT EXISTS analytics.fact_impressions_optimized (     impression_id UInt32 CODEC(Delta, LZ4),     campaign_id UInt32 CODEC(Delta, LZ4),     event_date Date CODEC(DoubleDelta, LZ4),     event_time DateTime CODEC(DoubleDelta, LZ4),     created_at DateTime CODEC(DoubleDelta, LZ4) ) ENGINE = MergeTree() PARTITION BY toYYYYMM(event_date) ORDER BY (campaign_id, event_date) SETTINGS index_granularity = 8192;   5. Parallel Processing for Transformations  Implement parallel transformation processing for CPU-intensive operations:  def transform_impressions_parallel(self, rows: List[Tuple]) -&gt; List[Tuple]:     \"\"\"Transform impression data using parallel processing.\"\"\"     from concurrent.futures import ProcessPoolExecutor          # Split data into chunks for parallel processing     chunk_size = 10000     chunks = [rows[i:i + chunk_size] for i in range(0, len(rows), chunk_size)]          # Process chunks in parallel     with ProcessPoolExecutor() as executor:         results = list(executor.map(self._transform_impression_chunk, chunks))          # Combine results     return [item for sublist in results for item in sublist]   Future-Proofing the Architecture  As digital advertising continues to evolve, this ETL pipeline can be extended in several ways:  1. Streaming Data Processing  Implement real-time data processing by integrating with streaming platforms:  class StreamingETLPipeline:     \"\"\"Real-time streaming ETL pipeline for advertising events.\"\"\"          def __init__(self, kafka_config, clickhouse_connector):         self.consumer = KafkaConsumer(             'adtech.events',             bootstrap_servers=kafka_config['bootstrap_servers'],             group_id='etl-consumer-group',             auto_offset_reset='earliest'         )         self.clickhouse = clickhouse_connector              def process_streaming_events(self):         \"\"\"Process streaming events from Kafka.\"\"\"         for message in self.consumer:             event = json.loads(message.value.decode('utf-8'))                          # Process different event types             if event['type'] == 'impression':                 self.process_impression(event)             elif event['type'] == 'click':                 self.process_click(event)                      def process_impression(self, event):         \"\"\"Process impression event and load to ClickHouse.\"\"\"         # Transform and load impression data         self.clickhouse.execute_query(             \"INSERT INTO analytics.fact_impressions VALUES\",             [(                 event['id'],                 event['campaign_id'],                 datetime.fromisoformat(event['timestamp']).date(),                 datetime.fromisoformat(event['timestamp']),                 datetime.now()             )]         )   2. Machine Learning Feature Store Integration  Extend the pipeline to support ML feature generation for predictive advertising:  class FeatureStoreLoader:     \"\"\"Loads transformed data into a feature store for ML applications.\"\"\"          def __init__(self, clickhouse_connector, feature_store_client):         self.clickhouse = clickhouse_connector         self.feature_store = feature_store_client              def generate_campaign_features(self):         \"\"\"Generate and load campaign performance features.\"\"\"         # Extract features from ClickHouse         features_data = self.clickhouse.execute_query(\"\"\"             SELECT                  campaign_id,                 toDate(event_time) AS day,                 count() AS daily_impressions,                 sum(case when exists(                     SELECT 1 FROM analytics.fact_clicks c                      WHERE c.campaign_id = i.campaign_id AND c.event_date = i.event_date                 ) then 1 else 0 end) AS daily_clicks,                 avg(bid) AS avg_bid             FROM analytics.fact_impressions i             JOIN analytics.dim_campaign c ON i.campaign_id = c.campaign_id             GROUP BY campaign_id, day             ORDER BY campaign_id, day         \"\"\")                  # Load to feature store         self.feature_store.ingest_features(             feature_group=\"campaign_daily_performance\",             features=features_data         )   3. Multi-Tenant Architecture  Scale the system to support multiple advertising platforms through tenant isolation:  class MultiTenantETLPipeline(ETLPipeline):     \"\"\"ETL pipeline with tenant isolation support.\"\"\"          def __init__(self, tenant_id, *args, **kwargs):         super().__init__(*args, **kwargs)         self.tenant_id = tenant_id              def extract_tenant_data(self, table, since):         \"\"\"Extract data specifically for this tenant.\"\"\"         query = f\"\"\"             SELECT * FROM {table}              WHERE tenant_id = %s AND updated_at &gt; %s         \"\"\"         return self.extractor.db.execute_query(query, (self.tenant_id, since))              def load_tenant_data(self, table, data):         \"\"\"Load data with tenant isolation.\"\"\"         # Ensure tenant_id is included in all loaded data         tenant_data = [(self.tenant_id,) + row for row in data]         return self.loader.load_generic(table, tenant_data)   Conclusion  Building this ETL pipeline for AdTech analytics was a structured exercise in balancing performance, reliability, and maintainability. The modular design allows for easy extension to additional data sources or targets, while the ClickHouse optimizations ensure fast query responses for critical KPIs.  While the current implementation meets the immediate needs, Iâ€™ve outlined clear paths for scaling and optimizing as data volumes grow. The real power of this architecture lies in its flexibilityâ€”it can evolve from batch processing to streaming, accommodate multi-tenant requirements, and integrate with advanced analytics platforms as business needs change.  For organizations starting similar projects, I recommend taking an incremental approach: begin with the core batch ETL functionality, validate the data model with real queries, then progressively enhance with optimizations and scalability features based on actual performance metrics and growth patterns.  This architectural pattern has proven highly effective for advertising analytics, where the ability to process billions of events while maintaining query performance is critical to driving business value through data-driven decision making."
  },
  
  {
    "title": "Takeaway: Unit Testing of Bash Scripts",
    "url": "/posts/takeaway-unit-testing-of-bash-scripts/",
    "categories": "Learning Journey",
    "tags": "bash, unittest",
    "date": "2023-09-28 13:40:00 +0200",
    "content": "Bash/Shell is a potent scripting language that lets us communicate with our computerâ€™s operating system. In many of my everyday tasks, I rely on Bash to carry out Linux commands and create certain logical processes. While Bashâ€™s capabilities are impressive, itâ€™s important to note that since it deals with low-level operations within the operating system, it can sometimes escalate minor problems into major ones, without any chance to revert the execution.  To ensure the reliability of our scripts for daily business operations, itâ€™s crucial to incorporate unit testing into our Bash scripts. This practice helps us maintain stability and prevent unexpected issues from affecting our workflow.  Briefly write down my takeaway from Chai Fengâ€™s sharing: https://www.youtube.com/live/-1gB_5dV32U?si=4uICRRG8vDmCKWxE&amp;t=649  There are 3 main barriers that prevent us from implementing Bash unit tests.    Dependencies, having the exact execution environment can be quite troublesome.   The script could have potential side effects and can be tricky to set up or tear down.   Command execution is the aim of the script, therefore itâ€™s hard to foresee the result.   Execution/validation is not fast enough.   Letâ€™s revisit the concept of scriptâ€™s unit tests. What exactly are they?  Is it validating whether the function called runs smoothly? Or to validate the execution logic of the indicated script? It doesnâ€™t matter running commands manually or by a script if the functions resulting errors. So the concept to implement scriptsâ€™ unit testing should be to validate the logic, instead of testing if the function can be run.  So what should we consider next? The methodology in Bash is essentially as same as other languages unit testing.    Donâ€™t bother executing the commands in the PATH because they are external dependencies of the script.   Each test instance is independent.   Different platforms, different environments, same results.   The validation action we expect is to execute the expected command, and send the expected parameters.   There are 5 things that determine the execution of commands. In order, they are aliases, keywords, functions, built-in procedures, and external procedures. The most important thing to consider about is how to simulate the command?  The testing framework for bash scripts: https://github.com/bach-sh/bach"
  },
  
  {
    "title": "Learning and Takeaways from Kubesimplify Workshop",
    "url": "/posts/learning-and-takeaways-from-kubesimplify-workshop/",
    "categories": "Learning Journey",
    "tags": "devops, linux, docker, k8s",
    "date": "2022-08-07 00:03:00 +0200",
    "content": "This post will keep the learning process and takeaways from the Kubesimplify workshops held by Saiyam Pathak. The motivation for me to catch up on this DevOps topic is to systematically learning by practising for DevOps mindset, in order to achieve double blade stacks, facilitating Data Engineering tasks and projects via DevOps approaches.  Workshops List    Linux &amp; Docker Fundamentals (Ongoing)   Kubernetes 101   GitOps With ArgoCD   Kubernetes Security 101   Kubernetes Troubleshooting   Linux &amp; Docker Fundamentals Instructor: Chad M. Crowell     Lecture&amp;Workshop recording   Learning resource   Linux fundamental Iâ€™ve been familiar with most of the commands introduced in this session. I still learnt something new because my learning before wasnâ€™t systematical enough. The takeaways for me in this session are listed below:     Knowing the naming and how Linux filesystem works in a clear picture.    Pressing control + r in the prompt is able to search historical executions. Under z-shell, thereâ€™s a way fancier drop-down list for a more intuitive search.   Using Command man to look up the manual of each command. -h isnâ€™t supported for every command, learnt how to use man is a great finding for me.   Linux commands are case-sensitive.   Simple command without switch on editor     echo 'var=\"something\"' &gt; file       # To overwrite the file echo \"var=\"something\"' &gt;&gt; file      # To append to the file           Create an intermediate directory with -p flag while using mkdir, for example     mkdir test/sub-test/error           # The prompt would pop out error mkdir -p test/sub-test/correct      # Successful execution           chmod commands usage https://chmodcommand.com/chmod-600/   To be continuedâ€¦"
  },
  
  {
    "title": "Implementing Fivetran Data Source Connector with AWS Lambda",
    "url": "/posts/implementing-fivetran-data-source-connector-with-aws-lambda/",
    "categories": "Data Engineering",
    "tags": "aws, lambda, fivetran, etl",
    "date": "2022-05-28 22:47:00 +0200",
    "content": "Basic background of AWS Lambda  Official developer guide from AWS  AWS Lambda is a serverless, event-driven compute service that lets you run code for virtually any type of application or backend service without provisioning or managing servers. In GCP and Azure, we can implement the same idea via Cloud Functions and Azure Funtions.  Why is it necessary to understand Lambda to build Fivetran connectors?  Fivetran is a reliable data pipeline platform for business users to connect their data source in a convenient way. It provides tons of connectors and integrations for user to choose, like marketing tools, modern databases, etc. Although Fivetran supports amounts of external APIs and data sources integration in default, some of the external APIs we needin reality which doesnâ€™t be supported by Fivetran directly.  If you are using AWS, then Lambda stands out at this moment! It allows us to write custom integration functions in Python, Node.js, etc. to approach data integration which not directly supported by Fivetran. In the following use case, connector/ETL was built in Python, connecting an API and Snwoflake. But this article would only focus on the less mentioned parts of the official Fivetran documentation. The basic architecture is shown in the figure below.     Prerequisite  Follow the instruction from Fivetran to setup the configuration, click here  Lambdaâ€™s sample function from Fivetranâ€™s document  import json def lambda_handler(request, context):     # Fetch records using api calls     (insertTransactions, deleteTransactions, newTransactionCursor) = api_response(request['state'], request['secrets'])         # Populate records in insert         insert = {}         insert['transactions'] = insertTransactions         delete = {}     delete['transactions'] = deleteTransactions         state = {}     state['transactionsCursor'] = newTransactionCursor         transactionsSchema = {}     transactionsSchema['primary_key'] = ['order_id', 'date']         schema = {}     schema['transactions'] = transactionsSchema         response = {}         # Add updated state to response     response['state'] =  state         # Add all the records to be inserted in response     response['insert'] = insert         # Add all the records to be marked as deleted in response     response['delete'] = delete         # Add schema defintion in response     response['schema'] = schema         # Add hasMore flag     response['hasMore'] = 'false'         return response\t def api_response(state, secrets):     # your api call goes here     insertTransactions = [             {\"date\":'2017-12-31T05:12:05Z', \"order_id\":1001, \"amount\":'$1200', \"discount\":'$12'},             {\"date\":'2017-12-31T06:12:04Z', \"order_id\":1001, \"amount\":'$1200', \"discount\":'$12'},     ]         deleteTransactions = [             {\"date\":'2017-12-31T05:12:05Z', \"order_id\":1000, \"amount\":'$1200', \"discount\":'$12'},             {\"date\":'2017-12-31T06:12:04Z', \"order_id\":1000, \"amount\":'$1200', \"discount\":'$12'},     ]         return (insertTransactions, deleteTransactions, '2018-01-01T00:00:00Z')   Multi-pages response implementation  Fivetran will stop the request if it gets a response has an attribute hasMore which equals 'false'  response['hasMore'] = 'false'   Which means, more than 1 pages response should be able to switch the value by a pointer. false should be able to altered to make Fivetran know that the request hasnâ€™t finished, the pointer should be updated as well once all pages are done. Iâ€™m sharing my implementation below to meet this requirement.  import datetime import asyncio from services import processor, cursor_formatter   def handler(event, context):     \"\"\"     Lambda function handler to handle output format     \"\"\"     loop = asyncio.get_event_loop()     insertLoanApplication, insertApplicants, insertAdvisors, state = \\         loop.run_until_complete(api_response(event['state'], event['secrets']))      if not state['moreData']:         loop.close()      insert = {         'loan_applications': insertLoanApplication,         'applicants': insertApplicants,         'advisors': insertAdvisors     }      schema_loan_applications = {'primary_key': ['id']}     schema_applicants = {'primary_key': ['loan_application_id', 'id']}     schema_advisors = {'primary_key': ['id']}     schema = {         'loan_applications': schema_loan_applications,         'applicants': schema_applicants,         'advisors': schema_advisors     }      return {         'state': state if state['moreData'] else {'cursor': cursor_formatter(datetime.datetime.now()),                                                   'page': 0},         'insert': insert,         'schema': schema,         'hasMore': 'false' if not state['moreData'] else 'true'     }   async def api_response(state, secrets):     \"\"\"     Main function to call indicated API     :param state: Fivetran anchor for indexing usage, default None     :param secrets: The secret you would like to use to call the API     :return: API responses     \"\"\"     try:         cursor_value, page = state['cursor'], state[\"page\"]     except KeyError:         cursor_value, page = cursor_formatter(datetime.datetime.now()), 0     page += 1     insertLoanApplication, insertApplicant, insertAdvisor, moreData = await processor(secrets, page, cursor_value)     state = {'cursor': cursor_value, 'page': page, 'moreData': moreData}      return insertLoanApplication, insertApplicant, insertAdvisor, state   First, we can see in api_request function, state is assigned by request format, empty dictionary object in default, we can assign any pointer we need for requesting API, see here to check the detail.  We retrieve the cursor and page at the beginning, cursor is for locating the timestamp of each response whether weâ€™ve done already, and page is literally for locating which page we are at. Fivetran can tell if this is an initial request, or if it is a request that is still pending and should be continued.  try:     cursor_value, page = state['cursor'], state[\"page\"] except KeyError:     cursor_value, page = cursor_formatter(datetime.datetime.now()), 0   After processing, we will get a function return value moreData for the Lambda handler to continue or stop the request.  state = {'cursor': cursor_value, 'page': page, 'moreData': moreData}   In this example, we have the return value from the handler to present continuing or stopping. Else we have no more new response from API, returning a timestamp and reset the page value to 0 for the next round requesting.  return {     'state': state if state['moreData'] else {'cursor': cursor_formatter(datetime.datetime.now()), 'page': 0},     'insert': insert,     'schema': schema,     'hasMore': 'false' if not state['moreData'] else 'true' }   Conclusion  Since the usage context is relatively small, Fivetran does not have document that describes how to request a multi-page response from API requesting, so we use several pointers to implement the requirements alone, which still quite fits actually.  If youâ€™ve noticed, Iâ€™ve also designed asynchronous requests for this purpose to speed up request efficiency, but this also creates a burden on the API server, Iâ€™ll share how to optimize requests in a later post."
  },
  
  {
    "title": "Useful gadget sharing - cron-job.org",
    "url": "/posts/useful-gadget-sharing-cron-job-org/",
    "categories": "Sharing",
    "tags": "heroku, cron",
    "date": "2021-04-30 02:00:00 +0200",
    "content": "Useful gadget sharing - cron-job.org  Due to initiating to maintain my side projects which have done before, I started investigating any pain points that needed to be improved or could be divided into small projects.  Story  I want to introduce a really cool, simple but useful gadget for you, cron-job.org.  The context was that I had a side project which deployed on Heroku. Heroku is a really convenient platform that provides a basic deployment environment for someone who wants to build their website or front-end app interface.  In short, Heroku free tier is quite sufficient for me. But your app is put to down after 30 mins of inactivity, and will need around 5â€“10 seconds to wake up the app again. If your app cannot tolerate that in any circumstance, you may want to try some approaches to avoid your app sleep again but still free. Pinging your app on a set interval might be an economy way to achieve it.  Approach     Make sure you have a simple GET request to your app homepage. For example, if your app is hosted at your-app.herokuapp.com, make sure you have a GET method handling â€œ/â€ route. Or, have a GET method at some other route.   Once thatâ€™s completed, we can either handle pinging our own app manually or automate the process. The Heroku free tier requires applications to sleep a minimum of seven hours every day. Unverified accounts get 550 of free dyno hours per month, which is equal to almost 7 hours of sleep time for a month of 31 days. To be on the safe side, weâ€™ll go with seven hours of sleep.   The example GET function writing in Python/Flask   @bp.route('/ping', methods=['GET']) def ping():     res = {         \"name\": \"CV parser\",         \"requested_at\": datetime.datetime.now(),         \"status\": \"ok\"     }     return jsonify(res)   Using cron-job.org to keep your app alive     Signup on https://cron-job.org/en/   Verify your email once the signup is done.   Log in and click on the Cronjobs tab. And click on â€œCreate cronjobâ€.   Fill in the details. Enter URL according to your GET method. For example, my URL will be https://you.herokuapp.com/ping     Schedule the execution time from 8 to 23 every 30 minutes every day     You can also check the execution detail in dashboard"
  },
  
  {
    "title": "Run your own Apache Spark jobs in AWS EMR and S3",
    "url": "/posts/run-your-own-apache-spark-jobs-in-aws-emr-and-s3/",
    "categories": "Data Engineering",
    "tags": "aws, datalake, pipeline",
    "date": "2021-02-01 01:00:00 +0100",
    "content": "Run your own Apache Spark jobs in AWS EMR and S3  Recently, I participated in Udacityâ€™s Nanodegree Program for Data Engineers. Itâ€™s kinda like to review what I did past and refresh some tech stacksâ€™ knowledge for me in the Data Engineering field. Today, Iâ€™m gonna writing this article to avoid you spending extra efforts to run a Spark job or other Hadoop eco-systems jobs on AWS.  This idea comes from the Data Lake demo below, you can find the part I tried to apply through Shell script and Makefile between [S3-&gt;EMR-&gt;S3] in the figure. The PySpark script I used reference from the project in Course Data Lake, click the photo to check detail.    Before we start, make sure youâ€™ve already done with registering an AWS account and downloading the credential on AWS console(the easiest way, you bet).  EMR workflow First off, let us have a look at the EMR workflow, click the photo to check detail.    What we want to do, is to make sure the work of setting up and launching a cluster, adding working steps, creating log files, and terminating the cluster as simple as we can. AWS CLI(command-line interface) builds an easy interface and functional usage for operating various services on AWS. But we are eager to make entire management much simpler and flexible to check the status, even put it aside until the job is finished.  Of course, you have other choices like SDK for different languages, like Python SDK or Java SDK if you like. In my experiences, every SDK sometimes use a different version of libraries or packages, you should be aware of the supporting version on each service you choose.  Idea and Practice We can choose the easy-checking cli command for pipeline, such as creating-cluster, add-steps, and terminate-clusters. The cli command from document is shown below. aws emr create-cluster \\ --name \"My First EMR Cluster\" \\ --release-label emr-5.31.0 \\ --applications Name=Spark \\ --ec2-attributes KeyName=myEMRKeyPairName \\ --instance-type m5.xlarge \\ --instance-count 3 \\ --use-default-roles   Set the command as a variable to load in the next step, add 2 additional arguments --query '[ClusterId]', --output text to change the output format and query the information we need, reference the variable cluster_id in the Shell scripts as below. cluster_id=\"$(aws emr create-cluster --name SparkifyEMR \\ --release-label emr-5.31.0 \\ --log-uri \"s3://$clustername/logs/\" \\ --applications Name=Spark \\ --ec2-attributes KeyName=\"$keyname\" \\ --instance-type m5.xlarge \\ --instance-count 3 \\ --use-default-roles \\ --query '[ClusterId]' \\ --output text)\" echo -e \"\\nClusterId: $cluster_id\"   Set the second variable step_id and query it. step_id=\"$(aws emr add-steps \\ --cluster-id \"$cluster_id\" \\ --steps \"Type=Spark,Name=SparkifyETL,ActionOnFailure=CONTINUE,\\ Args=[s3://$clustername/run_etl.py,--data_source,$clustername/data/,--output_uri,$clustername/output/]\" \\ --query 'StepIds[0]' \\ --output text)\" echo -e \"\\nStepId: $step_id\"   Set a while loop to check Spark jobs is finished or not. describe_step=\"$(aws emr describe-step --cluster-id \"$cluster_id\" \\   --step-id \"$step_id\" --query 'Step.Status.State' --output text)\"  while true; do   if [[ $describe_step = 'PENDING' ]]; then     echo \"Cluster creating... Autocheck state 2 mins later\"     sleep 120   elif [[ $describe_step = 'RUNNING' ]]; then     echo \"Job running... Autocheck state 30 seconds later\"     sleep 30   elif [[ $describe_step = 'FAILED' ]]; then     echo \"Job failed\"     break   else     echo \"Job completed\"     break   fi done   After finishing all jobs, terminate the cluster of the jobs complete without error. terminate_cluster=\"$(aws emr terminate-clusters \\ --cluster-ids \"$cluster_id\" \\ --query 'Cluster.Status.State' \\ --output text)\"  while true; do   if [[ $terminate_cluster != 'TERMINATED' ]]; then     echo \"Cluster terminating...\"     sleep 15   else     echo \"Cluster terminated\"   fi done   Conclusion Anyway, this script brings you a convenient approach to run one-time Spark jobs in AWS EMR Cluster. You can also check the all lines in the script here for reference.  if youâ€™d like to try it yourself, follow the tutorial from AWS official documents to build and manage your EMR clusters and jobs."
  },
  
  {
    "title": "How to train a customized Name Entities Recognition (NER) model based on spaCy pre-trained model",
    "url": "/posts/how-to-train-a-customized-name-entities-recognition-ner-model-based-on-spacy-pre-trained-model/",
    "categories": "Learning Journey",
    "tags": "ner, spacy, nlp",
    "date": "2020-03-04 01:00:00 +0100",
    "content": "How to train a customized Name Entities Recognition (NER) model based on spaCy pre-trained model  There are a bunch of online resources to teach you how to train your own NER model by spaCy, so I will attach some links for reference and skip this part.  spaCy: Training the named entity recognizer Convert SpaCy training data to Dataturks NER JSON output Custom Named Entity Recognition Using spaCy  Back to our main point of this article, and letâ€™s check the following code. Iâ€™m trying to train a customized NER model and retain the other pipeline in spaCy, i.e. tagger and parser. You can also add your own pipeline into the model as well.  Write a function as below, and load the model if youâ€™ve pointed it out. I used default pre-trained model en_core_web_sm here. def train_spacy(self, training_data, testing_data, dropout, display_freq=1,                  output_dir=None, new_model_name=\"en_model\"):     # create the built-in pipeline components and add them to the pipeline     # nlp.create_pipe works for built-ins that are registered with spaCy     # create blank Language class          if self.model:         nlp = spacy.load(self.model)     else:         nlp = spacy.blank('en')   The following is just like the tutorial from spaCy official website, create the pipeline or grab it, then add labels from the training data youâ€™ve annotated already.     if 'ner' not in nlp.pipe_names:         ner = nlp.create_pipe('ner')         nlp.add_pipe(ner, last=True)     else:         ner = nlp.get_pipe('ner')      # add labels     for _, annotations in training_data:         for ent in annotations.get('entities'):             ner.add_label(ent[2])   And now, letâ€™s training. The initial loss as 100000, and I set up the early stop to avoid overfitting.     # get names of other pipes to disable them during training     other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']     with nlp.disable_pipes(*other_pipes):  # only train NER         optimizer = nlp.begin_training()         print(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  Training the model  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\\n\")          losses_best = 100000         early_stop = 0   Use stochastic gradient descent as optimizer with mini-batch configuration, get more insights here.         for itn in range(self.n_iter):             print(f\"Starting iteration {itn + 1}\")             random.shuffle(training_data)             losses = {}             batches = minibatch(training_data, size=compounding(4., 32., 1.001))             for batch in batches:                 text, annotations = zip(*batch)                 nlp.update(                     text,  # batch of texts                     annotations,  # batch of annotations                     drop=dropout,  # dropout - make it harder to memorise data                     sgd=optimizer,  # callable to update weights                     losses=losses)              if itn % display_freq == 0:                 print(f\"Iteration {itn + 1} Loss: {losses}\")              if losses[\"ner\"] &lt; losses_best:                 early_stop = 0                 losses_best = int(losses[\"ner\"])             else:                 early_stop += 1              print(f\"Training will stop if the value reached {self.not_improve}, \"                   f\"and it's {early_stop} now.\\n\")              if early_stop &gt;= self.not_improve:                 break          print(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  Finished training  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\")   After training, dump the model as a binary data to your on-premises disk. Put the last 10 lines inside the with nlp.disable_pipes(*other_pipes): will save the only ner pipeline. Otherwise, the entire model will be saved, thatâ€™s exactly what I need.         if output_dir:             path = output_dir + f\"en_model_ner_{round(losses_best, 2)}\"         else:             path = os.getcwd() + f\"/lib/inactive_model/en_model_ner_{round(losses_best, 2)}\"             os.mkdir(path)         if testing_data:             self.validate_spacy(model=nlp, data=testing_data)      with nlp.use_params(optimizer.averages):         nlp.meta[\"name\"] = new_model_name         bytes_data = nlp.to_bytes()         lang = nlp.meta[\"lang\"]         pipeline = nlp.meta[\"pipeline\"]      model_data = dict(bytes_data=bytes_data, lang=lang, pipeline=pipeline)      with open(path + '/model.pkl', 'wb') as f:         pickle.dump(model_data, f)   You can load the model by the following function to load your model by your specified path on your disk. def load_model(model_path):     with open(model_path + '/model.pkl', 'rb') as f:         model = pickle.load(f)     nlp = spacy.blank(model['lang'])     for pipe_name in model['pipeline']:         pipe = nlp.create_pipe(pipe_name)         nlp.add_pipe(pipe)     nlp.from_bytes(model['bytes_data'])     return nlp"
  },
  
  {
    "title": "Connect PostgreSQL in docker container with Azure Data Studio",
    "url": "/posts/connect-postgresql-in-docker-container-with-azure-data-studio/",
    "categories": "Learning Journey",
    "tags": "azure-data-studio",
    "date": "2020-02-05 01:00:00 +0100",
    "content": "Connect PostgreSQL in docker container with Azure Data Studio  Azure Data Studio is a cool product that can easily connect MySQL (if you already installed in your system) and show what in your database look like. What if we use PostgresSQL instead of MySQL, how should we start?  The 1st step, I decided to pull Postgresâ€™s docker image but not to install at root.  $ docker pull postgres   Second, run the container, remember to set the port for local connecting (the document didnâ€™t mention it) $ docker run --name postgres-docker -e POSTGRES_PASSWROD=secret_password -p 5432:5432 -d postgres   then you will get container id, also can check it by docker ps  CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                      NAMES ba01d334f4db        postgres              \"docker-entrypoint.sâ€¦\"   37 minutes ago      Up 37 minutes       0.0.0.0:5432-&gt;5432/tcp     postgres-docker     After running the container, we could start the next step, set up the connection in Azure Data Studio.  1. Install PostgreSQL plug-in. Choose the extensionâ€™s icon and search Postgres then install it.      2. Click new connection, choose PostgreSQL as your database.      3. Enter the required information.      4. Remember to set port in advanced setting. Enter the ip and port as same as you assigned to docker container.      5. Connect and start to work in the database.        So far, the most important part is that remember to assign port when you run the docker container and enter the correct connection details, then it can keep your life much easier."
  },
  
  {
    "title": "Setting both Celery and Flask inside the docker-compose",
    "url": "/posts/setting-both-celery-and-flask-inside-the-docker-compose/",
    "categories": "Learning Journey",
    "tags": "celery, flask, docker",
    "date": "2020-02-04 01:00:00 +0100",
    "content": "Setting both Celery and Flask inside the docker-compose  Due to the issue I need to resolve is that put the heavy task to background, then run it periodically or asynchronously. And the most important thing is that have to support Python. I found Celery, the excellent tool to implement distributed task queue by Python, can easily deal with request using asynchronous designing. It seems like a perfect solution for my issue, however, there has still a new problem arised, what is I need to focus on processing the task ran by Flask and packed in docker and running by docker compose, it is difficult to find all of these resources at the same time on the Internet. So letâ€™s started!    My article is referenced from the following resources:    Asynchronous Tasks with Flask and Redis Queue   The web-service for extracting dominant color from images.   Using Celery with Flask   Task schduling with Celery (Mandarin resource)     App Structure Displaying Flask_Celery_example | â”œâ”€â”€ api |   â”œâ”€â”€ __init__.py |   â”œâ”€â”€ app.py                |   â”œâ”€â”€ celery_app.py |   â”œâ”€â”€ config.py |   â”œâ”€â”€ Dockerfile |   â””â”€â”€ requirements.txt | â”œâ”€â”€ docker-compose.yml â””â”€â”€ README.md  Set up your Redis and Celery worker in docker-compose, the breaking point is to set up the celery workerâ€™s name well. version: \"3\"  services:   web:     container_name: web     build: ./api     ports:       - \"5000:5000\"     links:       - redis     depends_on:       - redis     environment:       - FLASK_ENV=development     volumes:       - ./api:/app    redis:     container_name: redis     image: redis:5.0.5     hostname: redis    worker:     build:       context: ./api     hostname: worker     entrypoint: celery     command: -A celery_app.celery worker --loglevel=info     volumes:       - ./api:/app     links:       - redis     depends_on:       - redis    What the Dockerfile in this example looks like. FROM python:3.6.5  WORKDIR /app  COPY requirements.txt ./  RUN pip install --no-cache-dir -r requirements.txt  COPY . .  EXPOSE 5000  CMD [ \"python\", \"./app.py\" ]   Initialize Celery worker and create asynchronous or scheduled task in celery_app.py import config from celery import Celery   # Initialize Celery celery = Celery(     'worker',      broker=config.CeleryConfig.CELERY_BROKER_URL,     backend=config.CeleryConfig.CELERY_RESULT_BACKEND )   @celery.task() def func1(arg):     ...     return ...    Run task by Flask and check its status in app.py import config from celery_app import func1 from flask import Flask, request, jsonify, url_for, redirect   # Your API definition app = Flask(__name__) app.config.from_object('config.FlaskConfig')  @app.route('/', methods=['POST']) def longtask():     task = func1.delay(arg)     return redirect(url_for('taskstatus', task_id=task.id))      @app.route('/status/&lt;task_id&gt;') def taskstatus(task_id):     task = func1.AsyncResult(task_id)     if task.state == 'PENDING':         time.sleep(config.SERVER_SLEEP)         response = {             'queue_state': task.state,             'status': 'Process is ongoing...',             'status_update': url_for('taskstatus', task_id=task.id)         }     else:         response = {             'queue_state': task.state,             'result': task.wait()         }     return jsonify(response)   if __name__ == '__main__':     app.run()    After finishing the all setting, we can run the script on terminal below to create container and run it. $ cd Flask_Celery_example $ docker-compose build $ docker-compose run   Conclusion The bottleneck in this case is that I could run the Flask, Redis, Celery separately in docker-compose at first, but if I want to run it with only one script then it failed. I had tried and error lots of times to finding the breaking point, the correct script to run Celery in docker-compose. Everything was clear after throughing this bottleneck. Thanks for reading my first article! Leave your message below my Facebook post if thereâ€™s still any further questions. See you then."
  }
  
]

