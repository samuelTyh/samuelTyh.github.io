[ { "title": "Free n8n Hosting: Leveraging Hugging Face Spaces and Supabase for Persistent Workflow Automation", "url": "/posts/free-n8n-hosting-leveraging-hugging-face-spaces-and-supabase-for-persistent-workflow-automation/", "categories": "Learning Journey", "tags": "n8n, automation, workflow, huggingface, supabase", "date": "2025-06-02 20:52:00 +0200", "snippet": "Introductionn8n, the fair-code workflow automation platform, has transformed how developers and businesses automate their processes. While self-hosting n8n typically requires dedicated infrastructure, this guide demonstrates an innovative approach: hosting n8n entirely free using Hugging Face Spaces for compute and Supabase for persistent storage.This architecture addresses a critical limitation of Hugging Face‚Äôs free tier‚Äîephemeral storage‚Äîby offloading workflow persistence to Supabase‚Äôs generous free database tier. The result is a production-ready n8n instance without infrastructure costs, perfect for individuals, small teams, or proof-of-concept deployments.Architecture OverviewThe solution leverages two complementary platforms:Hugging Face Spaces provides: Free Docker container hosting 2 vCPU and 16GB RAM (free tier) Public HTTPS endpoint Automatic container managementSupabase delivers: PostgreSQL database (500MB free tier) Row-level security Real-time subscriptions Built-in authenticationThis combination circumvents Hugging Face‚Äôs ephemeral storage limitation while maintaining the performance characteristics needed for reliable workflow automation.PrerequisitesBefore beginning, ensure you have: A Hugging Face account (free signup at huggingface.co) A Supabase account (free signup at supabase.com) Basic familiarity with Docker and environment variables Git installed locallyStep 1: Set Up Supabase DatabaseFirst, create the PostgreSQL backend that will store your n8n workflows, credentials, and execution history.Create a New Supabase Project Log into your Supabase dashboard Click ‚ÄúNew project‚Äù Configure your project: Name: n8n-backend Database Password: Generate a strong password (save this!) Region: Choose the closest to your primary users Wait for project provisioning (typically 2-3 minutes)Configure Database Scheman8n requires specific database tables and permissions. Navigate to the SQL editor in your Supabase dashboard and execute:-- Create n8n schemaCREATE SCHEMA IF NOT EXISTS n8n;-- Grant permissions to authenticated usersGRANT ALL ON SCHEMA n8n TO postgres;GRANT ALL ON ALL TABLES IN SCHEMA n8n TO postgres;GRANT ALL ON ALL SEQUENCES IN SCHEMA n8n TO postgres;-- Enable UUID extension for n8nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";Retrieve Connection DetailsFrom your Supabase project settings, collect: Host: Found under Settings ‚Üí Database Port: Typically 5432 Database name: Usually ‚Äòpostgres‚Äô User: ‚Äòpostgres‚Äô Password: The password you created earlierYour connection string will look like:postgresql://postgres:[YOUR-PASSWORD]@[YOUR-PROJECT-REF].supabase.co:5432/postgresStep 2: Create Hugging Face SpaceNow, set up the compute environment for n8n.Initialize Space Repository Navigate to huggingface.co/spaces Click ‚ÄúCreate new Space‚Äù Configure: Space name: n8n-workflow-automation SDK: Docker Visibility: Public (required for free tier) Clone the repository locally: git clone https://huggingface.co/spaces/[YOUR-USERNAME]/n8n-workflow-automationcd n8n-workflow-automation Create DockerfileCreate a Dockerfile that configures n8n with Supabase integration:FROM n8nio/n8n:latest# Install PostgreSQL client for better database compatibilityUSER rootRUN apk add --no-cache postgresql-client# Create data directory with proper permissionsRUN mkdir -p /home/node/.n8n &amp;&amp; \\ chown -R node:node /home/node/.n8nUSER node# Set working directoryWORKDIR /home/node# Expose n8n portEXPOSE 5678# Health checkHEALTHCHECK --interval=30s --timeout=5s --start-period=30s \\ CMD curl -f http://localhost:5678/healthz || exit 1# Start n8nCMD [\"n8n\", \"start\"]Configure Environment VariablesCreate a .env file template (don‚Äôt commit actual values):# Database ConfigurationDB_TYPE=postgresdbDB_POSTGRESDB_HOST=your-project.supabase.coDB_POSTGRESDB_PORT=5432DB_POSTGRESDB_DATABASE=postgresDB_POSTGRESDB_USER=postgresDB_POSTGRESDB_PASSWORD=your-passwordDB_POSTGRESDB_SCHEMA=n8n# n8n ConfigurationN8N_BASIC_AUTH_ACTIVE=trueN8N_BASIC_AUTH_USER=adminN8N_BASIC_AUTH_PASSWORD=your-admin-passwordN8N_HOST=0.0.0.0N8N_PORT=5678N8N_PROTOCOL=httpsWEBHOOK_URL=https://your-space.hf.space# Execution ConfigurationEXECUTIONS_MODE=regularEXECUTIONS_PROCESS=mainN8N_METRICS=false# SecurityN8N_ENCRYPTION_KEY=your-32-char-encryption-keyCreate Space ConfigurationAdd a README.md with YAML frontmatter:---title: n8n Workflow Automationemoji: üîÑcolorFrom: bluecolorTo: purplesdk: dockerpinned: false---# n8n Workflow Automation PlatformThis Space hosts a fully functional n8n instance with persistent storage via Supabase.## Features- Complete n8n workflow automation- Persistent workflow storage- Secure credential management- Webhook support- API integrations## AccessVisit the Space URL and log in with the configured credentials.Step 3: Deploy and ConfigureSet Hugging Face SecretsIn your Space settings, add these secrets: Navigate to Settings ‚Üí Variables and secrets Add each environment variable from your .env file Mark sensitive values (passwords, keys) as ‚ÄúSecret‚ÄùInitial DeploymentPush your configuration:git add .git commit -m \"Initial n8n configuration\"git push origin mainHugging Face will automatically build and deploy your container. Monitor the logs for any issues.Verify Database ConnectionOnce deployed, access your n8n instance and verify: Navigate to https://[your-space].hf.space Log in with your basic auth credentials Create a test workflow Restart the Space (Settings ‚Üí Restart) Confirm your workflow persistsStep 4: Configure n8n for Production UseEnable Webhook URLsFor webhook-triggered workflows, configure the public URL: In n8n settings, set Webhook URL to your Space URL Test with a simple webhook workflow: { \"nodes\": [ { \"name\": \"Webhook\", \"type\": \"n8n-nodes-base.webhook\", \"position\": [250, 300], \"webhookId\": \"test-webhook\", \"parameters\": { \"path\": \"test\", \"responseMode\": \"onReceived\", \"responseData\": \"allEntries\" } } ]} Set Up Credentials Encryptionn8n encrypts stored credentials using the N8N_ENCRYPTION_KEY. Generate a secure key:openssl rand -hex 32Update this in your Space secrets to ensure credential security.Configure Execution RetentionTo manage database growth within Supabase‚Äôs free tier, configure execution pruning:EXECUTIONS_DATA_SAVE_ON_ERROR=allEXECUTIONS_DATA_SAVE_ON_SUCCESS=noneEXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS=trueEXECUTIONS_DATA_MAX_AGE=168 # 7 daysOptimization and Best PracticesPerformance Tuning Connection Pooling: Configure n8n‚Äôs database connection pool: DB_POSTGRESDB_POOL_MIN=2DB_POSTGRESDB_POOL_MAX=10 Memory Management: Monitor Space metrics and adjust workflow complexity accordingly Webhook Response Times: Keep webhook workflows lightweight to avoid timeoutsSecurity Considerations Access Control: Always enable basic authentication or implement OAuth Network Security: Use Supabase‚Äôs connection pooler for additional security Credential Rotation: Regularly update passwords and API keys Audit Logging: Enable n8n‚Äôs audit logs for complianceBackup StrategyImplement regular backups despite the free tier limitations:# Weekly backup script (run locally)pg_dump -h your-project.supabase.co -U postgres -d postgres -n n8n &gt; backup_$(date +%Y%m%d).sqlAdvantages for n8n BeginnersThis setup offers several compelling benefits:Zero Infrastructure Costs No server hosting fees No database hosting costs No domain or SSL certificate expenses Perfect for learning and experimentationProduction-Ready Features HTTPS endpoint provided automatically Database backups via Supabase Scalable to paid tiers when needed Professional deployment practicesLearning Opportunities Understand containerized deployments Practice with PostgreSQL databases Explore webhook integrations Build real automation workflowsEasy Migration PathWhen ready to scale: Export workflows from n8n Backup Supabase database Deploy to any cloud provider Import data and continueTroubleshooting Common IssuesSpace SleepingHugging Face Spaces sleep after inactivity. Solutions: Use external monitoring to ping your Space Implement a scheduled workflow that runs regularly Upgrade to a paid Space for always-on availabilityDatabase Connection ErrorsIf n8n can‚Äôt connect to Supabase: Verify connection string formatting Check Supabase connection limits Ensure proper SSL mode configuration Review Space logs for detailed errorsWebhook TimeoutsFor long-running webhooks: Implement async processing patterns Use n8n‚Äôs ‚ÄúRespond to Webhook‚Äù node Break complex workflows into smaller piecesFuture EnhancementsAs you grow comfortable with this setup, consider: Custom Nodes: Build and deploy custom n8n nodes Multi-Instance: Run multiple n8n instances with shared database Advanced Monitoring: Integrate with Supabase‚Äôs real-time features API Gateway: Add rate limiting and authentication layersConclusionHosting n8n on Hugging Face Spaces with Supabase backend represents a paradigm shift in accessible workflow automation. This architecture eliminates traditional barriers to entry while maintaining professional-grade capabilities.For beginners, it provides a risk-free environment to explore automation possibilities. For experienced users, it offers a viable production platform for non-critical workflows. As the ecosystem evolves, expect tighter integrations and enhanced capabilities that further democratize workflow automation.The convergence of specialized platforms like Hugging Face and Supabase demonstrates the future of composable infrastructure‚Äîwhere developers assemble best-in-class services rather than managing monolithic deployments. This approach not only reduces operational overhead but accelerates innovation by allowing focus on business logic rather than infrastructure management.Start building your automation workflows today, and join the growing community leveraging free-tier infrastructure for real-world solutions." }, { "title": "Terraform for Data Engineers: Automating Your Data Infrastructure", "url": "/posts/terraform-for-data-engineers-automating-your-data-infrastructure/", "categories": "Learning Journey", "tags": "terraform, devops, hashicorp, iac", "date": "2025-05-06 21:34:00 +0200", "snippet": "As a data engineer, we‚Äôre all too familiar with the pain of manually provisioning data processing resources, dealing with inconsistent environments, and the nightmare of trying to recreate a failed data pipeline. Enter Terraform ‚Äì a powerful tool that lets us define our entire data infrastructure as code, making it versionable, repeatable, and automated.What is Terraform?Terraform is an infrastructure as code tool that allows you to define, provision, and manage cloud resources across providers like AWS, GCP, and Azure using a simple, declarative language. Instead of clicking through console UIs or writing custom scripts, you write configuration files that describe your desired infrastructure state, and Terraform makes it happen.What makes Terraform particularly valuable for data engineers is its ability to provision and manage all the components of modern data platforms ‚Äì from storage and compute resources to data warehouses, ETL services, and analytics tools ‚Äì using a consistent workflow.How Terraform Fits into Data EngineeringAs data infrastructure grows more complex, crossing multiple cloud platforms and including dozens of specialized services, the old approach of manual provisioning becomes untenable. Terraform addresses this by: Automating repetitive tasks - Set up data lakes, data warehouses, and compute clusters with code rather than click-ops Standardizing environments - Ensure development, staging, and production environments are identical Enabling infrastructure evolution - Version control your data infrastructure alongside your code Supporting collaboration - Let team members understand and contribute to infrastructure changesTerraform Basics for Data EngineersTerraform uses HashiCorp Configuration Language (HCL) for its configuration files. Here‚Äôs a simple example showing how to set up an AWS S3 bucket for data lake storage:provider \"aws\" { region = \"us-west-2\"}# Create an S3 bucket for our data lakeresource \"aws_s3_bucket\" \"data_lake\" { bucket = \"my-company-data-lake\" tags = { Environment = \"production\" Department = \"data-engineering\" }}# Set up bucket for analytics resultsresource \"aws_s3_bucket\" \"analytics_results\" { bucket = \"my-company-analytics-results\" tags = { Environment = \"production\" Department = \"data-engineering\" }}The Basic Terraform WorkflowWorking with Terraform follows a straightforward process: Write your configuration in .tf files Init your project with terraform init to download providers Plan changes with terraform plan to see what will be created/modified Apply with terraform apply to create the resources Destroy with terraform destroy when you‚Äôre doneThis workflow is particularly useful for data projects where you might need to spin up temporary analysis environments or test new pipeline architectures without committing to permanent infrastructure changes.Data Engineering Use Cases for TerraformLet‚Äôs dive into specific ways Terraform can solve your data engineering challenges:1. Cloud Data Warehouse ProvisioningSetting up data warehouses like Redshift, BigQuery, or Snowflake requires numerous configuration choices. With Terraform, you can define these settings as code:resource \"aws_redshift_cluster\" \"analytics_warehouse\" { cluster_identifier = \"analytics-warehouse\" database_name = \"analytics\" master_username = var.redshift_admin_user master_password = var.redshift_admin_password node_type = \"dc2.large\" cluster_type = \"multi-node\" number_of_nodes = 3 # Enable encryption and logging encrypted = true enhanced_vpc_routing = true logging { enable = true bucket_name = aws_s3_bucket.redshift_logs.bucket s3_key_prefix = \"redshift-logs/\" }}This approach enables you to: Version control your warehouse configuration Easily replicate the setup in development/testing environments Automate warehouse scaling based on workload patterns2. Data Lake InfrastructureModern data lakes involve many components beyond just storage. Terraform lets you provision the entire stack:# S3 storage with proper partitioning setupresource \"aws_s3_bucket\" \"data_lake\" { bucket = \"company-data-lake\"}# Configure Glue Catalog for data discoveryresource \"aws_glue_catalog_database\" \"data_catalog\" { name = \"data_lake_catalog\"}# Set up partitions and tablesresource \"aws_glue_crawler\" \"data_crawler\" { name = \"data-lake-crawler\" role = aws_iam_role.glue_role.arn database_name = aws_glue_catalog_database.data_catalog.name s3_target { path = \"s3://${aws_s3_bucket.data_lake.bucket}/raw-data/\" } schedule = \"cron(0 */12 * * ? *)\"}# Add Athena workgroup for SQL queriesresource \"aws_athena_workgroup\" \"analytics\" { name = \"data-engineering\" configuration { result_configuration { output_location = \"s3://${aws_s3_bucket.query_results.bucket}/athena-results/\" } }}3. Streaming Data InfrastructureData engineers often need to set up real-time data processing pipelines. Terraform makes this easier by managing the complete infrastructure:# Kafka cluster on MSKresource \"aws_msk_cluster\" \"event_streaming\" { cluster_name = \"data-events-stream\" kafka_version = \"2.8.1\" number_of_broker_nodes = 3 broker_node_group_info { instance_type = \"kafka.m5.large\" client_subnets = var.private_subnets security_groups = [aws_security_group.kafka_sg.id] storage_info { ebs_storage_info { volume_size = 1000 } } }}# Kinesis Firehose for stream delivery to S3resource \"aws_kinesis_firehose_delivery_stream\" \"event_delivery\" { name = \"event-delivery-stream\" destination = \"extended_s3\" extended_s3_configuration { role_arn = aws_iam_role.firehose_role.arn bucket_arn = aws_s3_bucket.data_lake.arn prefix = \"streaming-events/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/\" buffer_interval = 60 buffer_size = 64 }}4. Compute Resources for Data ProcessingSpin up and manage compute resources for data transformation jobs:# EMR cluster for Spark processingresource \"aws_emr_cluster\" \"data_processing\" { name = \"data-processing-cluster\" release_label = \"emr-6.5.0\" applications = [\"Spark\", \"Hive\", \"Presto\"] ec2_attributes { subnet_id = var.subnet_id emr_managed_master_security_group = aws_security_group.emr_master.id emr_managed_slave_security_group = aws_security_group.emr_slave.id instance_profile = aws_iam_instance_profile.emr_profile.arn } master_instance_group { instance_type = \"m5.xlarge\" } core_instance_group { instance_type = \"m5.xlarge\" instance_count = 4 } configurations_json = &lt;&lt;EOF [ { \"Classification\": \"spark\", \"Properties\": { \"maximizeResourceAllocation\": \"true\" } } ] EOF}5. Managed Airflow for OrchestrationSet up a fully managed Apache Airflow environment:resource \"aws_mwaa_environment\" \"data_orchestration\" { name = \"data-pipeline-orchestrator\" airflow_version = \"2.5.1\" source_bucket_arn = aws_s3_bucket.airflow_assets.arn dag_s3_path = \"dags/\" execution_role_arn = aws_iam_role.mwaa_execution.arn network_configuration { security_group_ids = [aws_security_group.mwaa_sg.id] subnet_ids = var.private_subnets } logging_configuration { dag_processing_logs { enabled = true log_level = \"INFO\" } scheduler_logs { enabled = true log_level = \"INFO\" } webserver_logs { enabled = true log_level = \"INFO\" } worker_logs { enabled = true log_level = \"INFO\" } } environment_class = \"mw1.medium\" min_workers = 2 max_workers = 5}Practical Terraform Tips for Data EngineersCreating Reusable Data Infrastructure ModulesOne of Terraform‚Äôs most powerful features is modularity. You can create reusable modules for common data infrastructure patterns:# Example module usagemodule \"data_warehouse\" { source = \"./modules/redshift-warehouse\" cluster_name = \"analytics-production\" node_count = 4 node_type = \"dc2.large\" vpc_id = module.vpc.vpc_id subnet_ids = module.vpc.private_subnets}module \"data_lake\" { source = \"./modules/s3-data-lake\" bucket_name = \"company-data-lake-${var.environment}\" enable_versioning = true lifecycle_rules = var.storage_lifecycle_rules}Managing Secrets for Data ConnectionsHandling credentials for databases, warehouses, and APIs is a common challenge. Terraform integrates with secrets management services:# Use AWS Secrets Manager for database credentialsresource \"aws_secretsmanager_secret\" \"warehouse_creds\" { name = \"data/warehouse/admin\"}resource \"aws_secretsmanager_secret_version\" \"warehouse_creds\" { secret_id = aws_secretsmanager_secret.warehouse_creds.id secret_string = jsonencode({ username = var.admin_username password = var.admin_password })}# Reference in your Redshift configurationresource \"aws_redshift_cluster\" \"warehouse\" { # ... other configuration master_username = jsondecode(data.aws_secretsmanager_secret_version.warehouse_creds.secret_string)[\"username\"] master_password = jsondecode(data.aws_secretsmanager_secret_version.warehouse_creds.secret_string)[\"password\"]}Testing Data Infrastructure ChangesBefore applying changes to production systems, you can validate them:# Validate syntax and structureterraform validate# Check formattingterraform fmt -check# See what will change before applyingterraform plan -out=changes.plan# Apply the validated changesterraform apply changes.planIntegration with CI/CD for Data ProjectsIntegrate Terraform with your existing CI/CD pipelines to automate infrastructure updates alongside data pipeline code:# Example GitHub Actions workflowname: Deploy Data Infrastructureon: push: branches: [main] paths: - 'terraform/**' - '.github/workflows/terraform.yml'jobs: terraform: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Setup Terraform uses: hashicorp/setup-terraform@v2 - name: Terraform Init run: terraform init working-directory: ./terraform - name: Terraform Plan run: terraform plan -out=tfplan working-directory: ./terraform - name: Terraform Apply if: github.ref == 'refs/heads/main' run: terraform apply -auto-approve tfplan working-directory: ./terraformGetting Started as a Data EngineerReady to bring infrastructure as code to your data engineering practice? Here‚Äôs how to begin: Start small - Automate a single component of your data platform first, like an S3 bucket or Redshift cluster Use existing modules - Explore the Terraform Registry for pre-built data infrastructure modules Adopt gradually - You don‚Äôt need to migrate everything at once; Terraform can manage resources alongside manually created ones Version control - Store your Terraform files in the same repository as your data pipeline code Collaborate - Share your Terraform configurations with your team to build a consistent approachConclusionFor data engineers, Terraform isn‚Äôt just another tool ‚Äì it‚Äôs a fundamental shift in how we work with infrastructure. By codifying our data platform, we eliminate manual errors, enable repeatable deployments, and build a foundation for continuous evolution.Whether we‚Äôre running data workloads on AWS, GCP, Azure, or across multiple clouds, Terraform provides a consistent interface to provision and manage the entire stack. This allows us to focus on what matters most: building robust data pipelines and extracting value from our organization‚Äôs data.The initial investment in learning Terraform pays dividends in reduced complexity, greater reliability, and the ability to scale our data infrastructure alongside our growing data needs." }, { "title": "Building a Production-Ready Data Pipeline with Airflow and dbt", "url": "/posts/building-a-production-ready-data-pipeline-with-airflow-and-dbt/", "categories": "Data Engineering", "tags": "python, postgres, airflow, dbt, etl", "date": "2025-04-28 14:30:00 +0200", "snippet": "In today‚Äôs data-driven business landscape, transforming raw operational data into actionable insights requires robust, scalable data pipelines. I recently designed and implemented a comprehensive data engineering solution for sales analytics that bridges raw transactional data to dimensional models. This post walks through the architecture, implementation decisions, and practical patterns that you can apply to your own data engineering projects.The Challenge: From Sales Transactions to AnalyticsThe core challenge was to build a pipeline that would: Ingest sales transaction data from multiple CSV sources Handle common data quality issues (missing values, inconsistent formats) Transform raw data into a dimensional model for analytics Implement incremental processing for efficiency Ensure reliability with comprehensive testing and error handlingArchitecture Design: Medallion Pattern ImplementationAfter evaluating several architectural approaches, I implemented a medallion architecture (also called multi-hop architecture), which organizes data through progressive refinement stages:Raw CSV Data ‚Üí Bronze Layer (raw.sales) ‚Üí Silver Layer (transformed.dim_*) ‚Üí Gold Layer (analytics.fact_sales)This layered approach provides several key advantages: Clear separation of concerns Progressive data quality improvement Complete data lineage traceability Flexibility to rebuild downstream layers without re-ingesting source dataDatabase Schema DesignI designed the following schema structure: Bronze Layer: Raw data storage with original values preserved raw.sales: Original CSV data with added metadata columns Silver Layer: Cleaned and transformed dimensional models transformed.dim_product: Product information transformed.dim_retailer: Retailer information transformed.dim_location: Location information transformed.dim_channel: Sales channel information transformed.dim_date: Date dimension with hierarchies transformed.fact_sales: Sales fact table with foreign keys and measures Gold Layer: Analytics-ready views and aggregates analytics.dim_*: Analytics-ready dimension views analytics.fact_sales: Optimized analytical fact table Implementation: Core Components1. Data Ingestion ModuleThe ingestion component follows a ‚Äúvalidate-first, then clean‚Äù pattern, which provides better visibility into data quality issues:def main(file_path): \"\"\" Main function to process and load data. New flow: Validate first, then clean only records that need cleaning. \"\"\" logger.info(f\"Starting data ingestion process for {file_path}\") try: # Read the CSV file logger.info(f\"Reading file: {file_path}\") df = pd.read_csv(file_path) logger.info(f\"Successfully read {len(df)} records from {file_path}\") # First validate the data as-is is_valid, invalid_indices = validate_data(df) if not is_valid: logger.info(\"Data validation failed. Applying cleaning steps...\") # Apply cleaning only after validation fails df = CleanData.apply_all_cleaners(df) # Re-validate after cleaning is_valid, invalid_indices = validate_data(df) if not is_valid: logger.warning(\"Data still contains invalid records after cleaning. Filtering them out.\") df = filter_invalid_records(df, invalid_indices) else: logger.info(\"Data cleaning resolved all validation issues.\") else: logger.info(\"Data passed validation without cleaning.\") # Check for duplicates deduplicated_df = detect_duplicates(df) # Only proceed with loading if we have records if len(deduplicated_df) &gt; 0: # Get database connection string connection_string = get_db_connection_string() # Load to raw schema records_loaded = load_to_raw(deduplicated_df, connection_string, file_path) logger.info(f\"Data ingestion complete. {records_loaded} records processed.\") return records_loaded else: logger.warning(\"No valid records to load after validation and deduplication.\") return 0 except Exception as e: logger.error(f\"Error in data ingestion process: {str(e)}\") raiseI encapsulated the cleaning operations in a dedicated class with specialized methods for each type of cleaning:class CleanData: \"\"\" A class for handling different types of data cleaning operations. Each method handles a specific type of cleaning. \"\"\" @staticmethod def handle_missing_values(df): \"\"\"Handle missing values in the DataFrame.\"\"\" logger.info(\"Cleaning: Handling missing values...\") df_cleaned = df.copy() df_cleaned['Location'] = df_cleaned['Location'].fillna('Unknown') return df_cleaned @staticmethod def clean_price_values(df): \"\"\"Clean price values by removing currency symbols.\"\"\" logger.info(\"Cleaning: Cleaning price values...\") df_cleaned = df.copy() # Handle price with currency notation df_cleaned.loc[df_cleaned['Price'].str.contains('USD', na=False), 'Price'] = \\ df_cleaned.loc[df_cleaned['Price'].str.contains('USD', na=False), 'Price'].str.replace('USD', '') # Strip whitespace df_cleaned['Price'] = df_cleaned['Price'].str.strip() return df_cleaned # More cleaning methods... @classmethod def apply_all_cleaners(cls, df): \"\"\"Apply all cleaning methods in sequence.\"\"\" logger.info(\"Starting comprehensive data cleaning...\") df_result = df.copy() df_result = cls.handle_missing_values(df_result) df_result = cls.standardize_data_types(df_result) df_result = cls.remove_whitespace_values(df_result) df_result = cls.clean_price_values(df_result) df_result = cls.clean_date_values(df_result) logger.info(f\"Comprehensive data cleaning complete. Processed {len(df_result)} rows.\") return df_result2. Data Transformation LayerAfter landing raw data, the transformation component converts it into a proper dimensional model:def process_sales_data(engine): \"\"\" Process and transform sales data from raw to fact table. \"\"\" try: # Get dimension lookups channel_ids = populate_dim_channel(engine) location_ids = populate_dim_location(engine) product_ids = populate_dim_product(engine) retailer_ids = populate_dim_retailer(engine) date_ids = populate_dim_date(engine) # Query to get raw sales data query = \"\"\" SELECT \"SaleID\", \"ProductID\", \"RetailerID\", \"Channel\", \"Location\", \"Quantity\", \"Price\", \"Date\" FROM raw.sales WHERE \"SaleID\" NOT IN ( SELECT sale_id::VARCHAR FROM transformed.fact_sales ) ORDER BY \"SaleID\" ASC \"\"\" with engine.begin() as conn: # Count total records to process count_query = \"\"\" SELECT COUNT(*) FROM raw.sales WHERE \"SaleID\" NOT IN ( SELECT sale_id::VARCHAR FROM transformed.fact_sales ) \"\"\" result = conn.execute(text(count_query)) total_records = result.fetchone()[0] logger.info(f\"Found {total_records} new sales records to process\") if total_records == 0: logger.info(\"No new records to process\") return 0 result = conn.execute(text(query)) sales = [dict(zip(result.keys(), row)) for row in result] # Transform data logger.info(f\"Processing {len(sales)} sales records\") processed_count = 0 fact_records = [] # Process each sale record for sale in sales: try: # Data transformations here... # Create fact record fact_record = { \"sale_id\": sale_id, \"product_id\": product_id, \"retailer_id\": retailer_id, \"location_id\": location_id, \"channel_id\": channel_id, \"date_id\": date_id, \"quantity\": quantity, \"unit_price\": unit_price, \"total_amount\": total_amount } fact_records.append(fact_record) except Exception as e: logger.error(f\"Error processing sale {sale['SaleID']}: {str(e)}\") # Insert fact records if fact_records: try: insert_query = \"\"\" INSERT INTO transformed.fact_sales ( sale_id, product_id, retailer_id, location_id, channel_id, date_id, quantity, unit_price, total_amount ) VALUES ( :sale_id, :product_id, :retailer_id, :location_id, :channel_id, :date_id, :quantity, :unit_price, :total_amount ) ON CONFLICT (sale_id) DO NOTHING \"\"\" # Use a new transaction to ensure atomicity with engine.begin() as insert_conn: insert_conn.execute(text(insert_query), fact_records) processed_count = len(fact_records) logger.info(f\"Inserted {len(fact_records)} records into fact_sales\") except Exception as e: logger.error(f\"Error inserting: {str(e)}\") logger.info(f\"Successfully processed {processed_count} sales records\") return processed_count except Exception as e: logger.error(f\"Error processing sales data: {str(e)}\") raise3. dbt Transformation ModelsFor the analytical layer, I implemented dbt models that further refine the data:First, a staging model to standardize raw data formats:-- stg_sales.sqlwith source as ( select * from {{ source('postgres', 'sales') }}),cleaned as ( select \"SaleID\"::integer as sale_id, nullif(\"ProductID\", '')::integer as product_id, \"ProductName\" as product_name, \"Brand\" as brand, \"Category\" as category, \"RetailerID\"::integer as retailer_id, \"RetailerName\" as retailer_name, \"Channel\" as channel, coalesce(nullif(\"Location\", ''), 'Unknown') as location, case when \"Quantity\" ~ '^-?\\d+$' then \"Quantity\"::integer else null end as quantity, case when \"Price\" ~ '^\\d+$' then \"Price\"::decimal when \"Price\" ~ '^\\d+USD$' then replace(\"Price\", 'USD', '')::decimal else null end as price, case when \"Date\" ~ '^\\d{4}-\\d{2}-\\d{2}$' then \"Date\"::date when \"Date\" ~ '^\\d{4}/\\d{2}/\\d{2}$' then to_date(\"Date\", 'YYYY/MM/DD') else null end as date, batch_id, source_file, inserted_at from source),final as ( select sale_id, product_id, product_name, brand, category, retailer_id, retailer_name, channel, location, case when quantity &lt;= 0 then null else quantity end as quantity, price, date, batch_id, source_file, inserted_at, current_timestamp as transformed_at from cleaned where sale_id is not null and product_id is not null and retailer_id is not null and date is not null and quantity is not null and price is not null)select * from finalThen dimensional models built on top of staging:-- fact_sales.sql{{ config( unique_key = 'sale_id', indexes = [ {'columns': ['sale_id'], 'unique': True}, {'columns': ['product_id']}, {'columns': ['retailer_id']}, {'columns': ['location_id']}, {'columns': ['channel_id']}, {'columns': ['date_id']} ] )}}with stg_sales as ( select * from {{ ref('stg_sales') }}),dim_product as ( select * from {{ ref('dim_product') }}),dim_location as ( select * from {{ ref('dim_location') }}),-- Create dimension references for retailer and channeldim_retailer as ( select distinct retailer_id, retailer_name from stg_sales),dim_channel as ( select channel, {{ dbt_utils.generate_surrogate_key(['channel']) }} as channel_id from stg_sales group by channel),-- Final fact sales tablefinal as ( select s.sale_id, s.product_id, s.retailer_id, l.location_id, c.channel_id, s.date as date_id, s.quantity, s.price / nullif(s.quantity, 0)::numeric(10, 2) as unit_price, s.price::numeric(12, 2) as total_amount, s.transformed_at from stg_sales s inner join dim_location l on l.location = s.location inner join dim_channel c on c.channel = s.channel {% if is_incremental() %} where s.transformed_at &gt; (select max(transformed_at) from {{ this }}) {% endif %})select * from final4. Orchestration with AirflowThe Airflow DAGs orchestrate the entire pipeline. I implemented two primary DAGs: The Sales Data Pipeline for ingestion and initial transformation:# Define task dependenciescheck_file_exists &gt;&gt; check_and_ingest_data &gt;&gt; transform_raw_data &gt;&gt; archive_file The dbt Transformation Pipeline for analytical models:# Define dbt commandsdbt_deps_cmd = f\"\"\"cd {DBT_PROJECT_DIR} &amp;&amp; dbt deps --profiles-dir {DBT_PROFILES_DIR} --target {DBT_TARGET}\"\"\"dbt_run_staging_cmd = f\"\"\"cd {DBT_PROJECT_DIR} &amp;&amp; dbt run --models \"staging.*\" --profiles-dir {DBT_PROFILES_DIR} --target {DBT_TARGET}\"\"\"dbt_run_marts_cmd = f\"\"\"cd {DBT_PROJECT_DIR} &amp;&amp; dbt run --models \"marts.*\" --profiles-dir {DBT_PROFILES_DIR} --target {DBT_TARGET}\"\"\"dbt_test_cmd = f\"\"\"cd {DBT_PROJECT_DIR} &amp;&amp; dbt test --profiles-dir {DBT_PROFILES_DIR} --target {DBT_TARGET}\"\"\"# Define task dependenciescheck_and_ingest_data &gt;&gt; install_dependencies &gt;&gt; run_staging_models &gt;&gt; run_mart_models &gt;&gt; test_modelsAdvanced FeaturesSelf-healing Data FlowA key feature of this pipeline is its ‚Äúself-healing‚Äù capability. Both DAGs automatically check if the required data exists before proceeding, and trigger upstream processes if needed:class RawDataSensor(BaseSensorOperator): \"\"\" Sensor to check if there's data in the raw.sales table. \"\"\" @apply_defaults def __init__(self, conn_id=\"sales_db\", *args, **kwargs): super().__init__(*args, **kwargs) self.conn_id = conn_id def poke(self, context): hook = PostgresHook(postgres_conn_id=self.conn_id) sql = \"SELECT COUNT(*) FROM raw.sales\" count = hook.get_first(sql)[0] self.log.info(f\"Found {count} rows in raw.sales table\") return count &gt; 0def check_and_ingest_data(csv_file_path, conn_id=\"sales_db\", **context): \"\"\" Check if data exists in raw.sales and ingest if empty. \"\"\" hook = PostgresHook(postgres_conn_id=conn_id) # Check if data exists sql = \"SELECT COUNT(*) FROM raw.sales\" count = hook.get_first(sql)[0] # If data exists, return True if count &gt; 0: context['ti'].xcom_push(key='data_already_exists', value=True) return True # If no data, perform ingestion try: ingest_main = import_ingest_module() records_loaded = ingest_main(csv_file_path) # Verify ingestion was successful if records_loaded &gt; 0: context['ti'].xcom_push(key='records_loaded', value=records_loaded) return True else: context['ti'].xcom_push(key='ingest_failed', value=True) return False except Exception as e: context['ti'].xcom_push(key='ingest_error', value=str(e)) raiseThis design enables either pipeline to be triggered independently without failures, creating a more resilient system.Data Quality TestingComprehensive data quality checks are implemented in both Python and dbt: Python validation for source data:def validate_data(df): \"\"\" Validate data quality and identify invalid records. Returns a boolean indicating if the data is valid and a list of invalid indices. \"\"\" logger.info(f\"Validating data... Total records: {len(df)}\") # Track invalid rows for logging invalid_rows = { 'dates': [], 'quantities': [], 'prices': [], 'all': set() # Use a set to avoid duplicates } # Check for invalid dates for idx, date_str in enumerate(df['Date']): try: # Try to parse the date if isinstance(date_str, str) and date_str: # Handle different date formats if '/' in date_str: datetime.strptime(date_str, '%Y/%m/%d') else: datetime.strptime(date_str, '%Y-%m-%d') else: # Empty or non-string date invalid_rows['dates'].append((idx, date_str)) invalid_rows['all'].add(idx) except ValueError: invalid_rows['dates'].append((idx, date_str)) invalid_rows['all'].add(idx) # More validation checks... return is_valid, list(invalid_rows['all']) dbt tests for transformed data:# schema.yml for fact_salesversion: 2models: - name: fact_sales description: \"Fact table for sales with related dimension keys\" columns: - name: sale_id description: \"The primary key for the sales transaction\" tests: - unique - not_null - name: product_id description: \"Foreign key to product dimension\" tests: - not_null - relationships: to: ref('dim_product') field: product_id # Additional column tests... - name: quantity description: \"Number of items sold\" tests: - not_null - positive_valuesIncremental ProcessingThe pipeline implements incremental processing at multiple levels: Python-based ETL uses ID-based tracking:# Query to get only new records query = \"\"\"SELECT \"SaleID\", \"ProductID\", \"RetailerID\", \"Channel\", \"Location\", \"Quantity\", \"Price\", \"Date\"FROM raw.salesWHERE \"SaleID\" NOT IN ( SELECT sale_id::VARCHAR FROM transformed.fact_sales)ORDER BY \"SaleID\" ASC\"\"\" dbt models use incremental materialization:{% if is_incremental() %}where s.transformed_at &gt; (select max(transformed_at) from {{ this }}){% endif %}This two-tiered approach ensures efficient processing of only new or changed data.Containerization and DeploymentThe entire solution is containerized using Docker for consistent deployment:services: postgres: image: postgres:latest environment: - POSTGRES_USER=postgres - POSTGRES_PASSWORD=mysecretpassword - POSTGRES_MULTIPLE_DATABASES=airflow,sales volumes: - ./initdb:/docker-entrypoint-initdb.d - postgres-db-volume:/var/lib/postgresql/data healthcheck: test: [\"CMD\", \"pg_isready\", \"-U\", \"postgres\"] interval: 5s retries: 5 ports: - \"5433:5432\" restart: always airflow-webserver: &lt;&lt;: *airflow-common command: webserver ports: - 8081:8080 healthcheck: test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8081/health\"] interval: 10s timeout: 10s retries: 5 restart: always # Additional services...Scalability ApproachesAs the data volume grows, several scalability enhancements can be implemented:1. Partitioning for Larger DatasetsFor larger datasets, implementing table partitioning in PostgreSQL can significantly improve performance:-- Example of adding partitioning to fact_salesCREATE TABLE analytics.fact_sales ( sale_id INTEGER, -- other columns... date_id DATE NOT NULL) PARTITION BY RANGE (date_id);-- Create partitions by monthCREATE TABLE fact_sales_2024_q1 PARTITION OF fact_sales FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');CREATE TABLE fact_sales_2024_q2 PARTITION OF fact_sales FOR VALUES FROM ('2024-04-01') TO ('2024-07-01');2. Parallel Processing with AirflowFor processing large volumes of data, implementing parallel task execution in Airflow:# Create parallel tasks for processing different data segmentsfor segment in get_data_segments(): process_segment = PythonOperator( task_id=f'process_segment_{segment}', python_callable=process_data_segment, op_kwargs={'segment': segment}, dag=dag, ) # Set dependencies check_and_ingest_data &gt;&gt; process_segment &gt;&gt; merge_segments3. Enhanced Incremental ProcessingThe current incremental approach can be enhanced with timestamp-based windowing:def extract_incremental_data(start_time, end_time, batch_size=10000): \"\"\"Extract data in time-bounded batches for efficient processing.\"\"\" current_position = start_time while current_position &lt; end_time: next_position = min(current_position + timedelta(hours=1), end_time) query = \"\"\" SELECT * FROM raw.sales WHERE inserted_at &gt;= %s AND inserted_at &lt; %s ORDER BY inserted_at LIMIT %s \"\"\" yield execute_query(query, (current_position, next_position, batch_size)) current_position = next_positionThis approach reduces memory pressure when dealing with large datasets.Optimization Techniques1. Database Indexing StrategyCarefully designed indexes dramatically improve query performance:-- Indexes for the fact tableCREATE INDEX idx_fact_sales_product_id ON transformed.fact_sales(product_id);CREATE INDEX idx_fact_sales_retailer_id ON transformed.fact_sales(retailer_id);CREATE INDEX idx_fact_sales_date_id ON transformed.fact_sales(date_id);CREATE INDEX idx_fact_sales_channel_id ON transformed.fact_sales(channel_id);2. Memory-Efficient ProcessingFor large datasets, implement batch processing to control memory usage:def process_large_dataset(file_path, batch_size=10000): \"\"\"Process large CSV files in batches to control memory usage.\"\"\" # Use pandas chunking for memory efficiency for chunk in pd.read_csv(file_path, chunksize=batch_size): # Validate and clean the chunk is_valid, invalid_indices = validate_data(chunk) if not is_valid: chunk = CleanData.apply_all_cleaners(chunk) chunk = filter_invalid_records(chunk, invalid_indices) # Process the cleaned chunk load_to_raw(chunk, get_db_connection_string(), file_path)3. Airflow Task ConfigurationOptimizing Airflow task configuration for better resource utilization:# Task configuration for better resource managementtask = PythonOperator( task_id='transform_raw_data', python_callable=transform_main, executor_config={ 'cpu_millicores': 1000, 'memory_mb': 2048, }, dag=dag,)ConclusionThis data pipeline demonstrates how modern tools and architectural patterns can create a robust, production-ready data infrastructure. By combining Airflow‚Äôs orchestration capabilities with dbt‚Äôs transformation power and a well-designed schema, we‚Äôve built a system that can handle real-world data challenges while maintaining flexibility for future growth.Key takeaways from this implementation: The value of a layered architectural approach (medallion pattern) The importance of separating validation from cleaning for better data quality management The benefits of self-healing data flows that can recover from failures How containerization provides environment consistency across development and productionWhile no data pipeline is ever truly ‚Äúcomplete‚Äù (data requirements evolve continuously), this implementation provides a solid foundation that can adapt to changing business needs. The patterns and practices demonstrated here can help create more resilient, maintainable data systems for organizations of any size.The complete code for this project is available on GitHub at samuelTyh/airflow-dbt-sales-analytics.Future WorkLooking ahead, this pipeline could be enhanced with: Real-time streaming capabilities: Integrating a streaming solution like Kafka for near real-time data processing Advanced data quality monitoring: Adding automated data quality monitoring with Great Expectations or dbt expectations ML feature engineering: Extending the pipeline to generate features for machine learning models Cloud-native deployment: Adapting the architecture for cloud platforms with services like AWS Glue, Azure Data Factory, or Google Cloud DataflowThese enhancements would further extend the capabilities of the pipeline while maintaining the core architectural principles that make it reliable and maintainable.What data pipeline patterns have you found most effective in your work? Share your thoughts and questions in the comments below!" }, { "title": "Building a Scalable ETL Pipeline for AdTech Analytics", "url": "/posts/building-a-scalable-etl-pipeline-for-adtech-analytics/", "categories": "Data Engineering", "tags": "python, postgres, etl, clickhouse", "date": "2025-04-21 17:47:00 +0200", "snippet": "In the world of digital advertising, data is everything. Transforming raw operational data into actionable insights requires robust analytics pipelines. Recently, I implemented a comprehensive ETL (Extract, Transform, Load) solution for an advertising platform that moves data from PostgreSQL to ClickHouse for high-performance analytics. Let me walk you through the process, design decisions, and implementation details.Overview of the ChallengeThe challenge was to build a pipeline that would: Extract campaign, impression, and click data from a PostgreSQL operational database Transform the data into an analytics-friendly format Load it into ClickHouse, a columnar DBMS optimized for analytical queries Create materialized views for key advertising KPIsThe solution needed to handle both initial data loads and incremental updates, with robust error handling and monitoring capabilities.Architecture DesignAfter reviewing the requirements, I designed the following architecture:PostgreSQL (Source) ‚Üí ETL Pipeline ‚Üí ClickHouse (Target)Source Database (PostgreSQL)The operational database contained four primary tables: advertiser: Information about companies running ad campaigns campaign: Campaign configurations with bid amounts and budgets impressions: Records of ads being displayed clicks: Records of users clicking on adsTarget Schema (ClickHouse)For the analytical layer, I designed a dimensional model with: Dimension tables: dim_advertiser dim_campaign Fact tables: fact_impressions fact_clicks Materialized views for KPIs: Campaign CTR (Click-Through Rate) Daily performance metrics Campaign daily performance Cost efficiency metrics Advertiser performance overviews Implementation Details1. Setting Up the Core ComponentsI implemented the ETL pipeline in Python 3.12 with a modular design to separate concerns:from .config import AppConfig, PostgresConfig, ClickhouseConfig, ETLConfigfrom .db import PostgresConnector, ClickhouseConnectorfrom .schema import SchemaManagerfrom .pipeline import ( DataExtractor, DataTransformer, DataLoader, ETLPipeline)2. Database ConnectorsThe first components I built were the database connectors, which encapsulate connection management and query execution:class PostgresConnector: \"\"\"PostgreSQL connection manager.\"\"\" def __init__(self, config: PostgresConfig): \"\"\"Initialize with PostgreSQL configuration.\"\"\" self.config = config self.conn = None def connect(self) -&gt; bool: \"\"\"Establish connection to PostgreSQL database.\"\"\" try: self.conn = psycopg.connect( self.config.connection_string, autocommit=False, ) logger.info(\"Connected to PostgreSQL database\") return True except Exception as e: logger.error(f\"Failed to connect to PostgreSQL: {e}\") return False # Additional methods for query execution, etc.Similarly, I implemented a ClickhouseConnector for managing ClickHouse connections:class ClickhouseConnector: \"\"\"ClickHouse connection manager.\"\"\" def __init__(self, config: ClickhouseConfig): \"\"\"Initialize with ClickHouse configuration.\"\"\" self.config = config self.client = None # Methods for connection, query execution, etc.3. Schema ManagementI created a SchemaManager class to handle ClickHouse schema setup and updates:class SchemaManager: \"\"\"Manages ClickHouse schema creation and updates.\"\"\" def __init__(self, db_connector: ClickhouseConnector, config: ETLConfig): \"\"\"Initialize with ClickHouse connector and configuration.\"\"\" self.db = db_connector self.config = config def setup_schema(self) -&gt; bool: \"\"\"Initialize ClickHouse schema if not exists.\"\"\" try: self.db.execute_file(self.config.schema_path) logger.info(\"ClickHouse schema initialized successfully\") return True except Exception as e: logger.error(f\"Error setting up ClickHouse schema: {e}\") return False4. The ETL Pipeline ComponentsThe core of the implementation consists of three main components:A. Data Extractorclass DataExtractor: \"\"\"Extracts data from PostgreSQL source database.\"\"\" def __init__(self, db: PostgresConnector): \"\"\"Initialize with PostgreSQL connector.\"\"\" self.db = db def extract_advertisers(self, since: datetime) -&gt; List[Tuple]: \"\"\"Extract advertisers updated since the given timestamp.\"\"\" query = \"\"\" SELECT id, name, updated_at, created_at FROM advertiser WHERE updated_at &gt; %s OR created_at &gt; %s \"\"\" return self.db.execute_query(query, (since, since)) # Additional methods for extracting campaigns, impressions, clicksB. Data Transformerclass DataTransformer: \"\"\"Transforms data for loading into ClickHouse.\"\"\" @staticmethod def transform_advertisers(rows: List[Tuple]) -&gt; List[Tuple]: \"\"\"Transform advertiser data for ClickHouse.\"\"\" transformed = [] for adv_id, name, updated_at, created_at in rows: transformed.append(( adv_id, name, updated_at or datetime.now(), created_at or datetime.now() )) return transformed # Additional transformation methodsC. Data Loaderclass DataLoader: \"\"\"Loads transformed data into ClickHouse.\"\"\" def __init__(self, db: ClickhouseConnector): \"\"\"Initialize with ClickHouse connector.\"\"\" self.db = db def load_advertisers(self, data: List[Tuple]) -&gt; int: \"\"\"Load advertiser data into ClickHouse.\"\"\" if not data: return 0 query = \"\"\" INSERT INTO analytics.dim_advertiser (advertiser_id, name, updated_at, created_at) VALUES \"\"\" self.db.execute_query(query, data) return len(data) # Additional loading methods5. Orchestrating the ETL ProcessI created an ETLPipeline class to orchestrate the entire process:class ETLPipeline: \"\"\"Main ETL pipeline that orchestrates extract, transform, and load.\"\"\" def __init__( self, extractor: DataExtractor, transformer: DataTransformer, loader: DataLoader ): \"\"\"Initialize with extractor, transformer, and loader components.\"\"\" self.extractor = extractor self.transformer = transformer self.loader = loader self.last_sync = { 'advertiser': datetime.min, 'campaign': datetime.min, 'impressions': datetime.min, 'clicks': datetime.min } # Tracking for sync statistics self.sync_stats = { 'advertiser': 0, 'campaign': 0, 'impressions': 0, 'clicks': 0 } def run_sync_cycle(self) -&gt; bool: \"\"\"Run a complete ETL cycle.\"\"\" try: logger.info(\"Starting ETL sync cycle\") # Reset sync statistics for key in self.sync_stats: self.sync_stats[key] = 0 # Sync dimension tables first self.sync_stats['advertiser'] = self.sync_advertisers() self.sync_stats['campaign'] = self.sync_campaigns() # Then sync fact tables self.sync_stats['impressions'] = self.sync_impressions() self.sync_stats['clicks'] = self.sync_clicks() # Log sync summary logger.info(\"ETL sync cycle completed successfully\") logger.info(f\"Sync summary: \" f\"Advertisers: {self.sync_stats['advertiser']}, \" f\"Campaigns: {self.sync_stats['campaign']}, \" f\"Impressions: {self.sync_stats['impressions']}, \" f\"Clicks: {self.sync_stats['clicks']}\") return True except Exception as e: logger.error(f\"ETL sync cycle failed: {e}\") return False6. Implementing Incremental UpdatesOne of the most critical aspects of the implementation was handling incremental updates efficiently. I designed the system to track the last sync timestamp for each entity:def sync_advertisers(self) -&gt; int: \"\"\"Sync advertisers from PostgreSQL to ClickHouse.\"\"\" try: # Extract only data updated since last sync rows = self.extractor.extract_advertisers(self.last_sync['advertiser']) if not rows: logger.info(\"No new advertisers to sync\") return 0 # Transform and load data = self.transformer.transform_advertisers(rows) count = self.loader.load_advertisers(data) # Update the last sync timestamp latest_update = self.last_sync['advertiser'] for _, _, updated_at, created_at in rows: if updated_at and updated_at &gt; latest_update: latest_update = updated_at if created_at and created_at &gt; latest_update: latest_update = created_at self.last_sync['advertiser'] = latest_update logger.info(f\"Synced {count} advertisers\") return count except Exception as e: logger.error(f\"Error syncing advertisers: {e}\") return 07. Main Service LoopFinally, I implemented a main service class to tie everything together:def run_service(self, run_once: bool = False, interval: Optional[int] = None, force_full_sync: bool = False) -&gt; None: \"\"\"Run the ETL service continuously or once.\"\"\" sync_interval = interval or self.config.etl.sync_interval if not self.initialize(): self.logger.error(\"Service initialization failed. Exiting.\") sys.exit(1) if force_full_sync: self.logger.info(\"Forcing full sync - resetting all sync timestamps\") for key in self.etl_pipeline.last_sync: self.etl_pipeline.last_sync[key] = datetime.min self.logger.info(f\"AdTech ETL service started with sync interval: {sync_interval}s\") try: if run_once: success = self.run_sync() if not success: sys.exit(1) else: # Continuous operation while True: success = self.run_sync() if not success: self.logger.warning(f\"Waiting {sync_interval} seconds before retry...\") self.logger.info(f\"Sleeping for {sync_interval} seconds...\") time.sleep(sync_interval) except KeyboardInterrupt: self.logger.info(\"ETL service interrupted, shutting down\") except Exception as e: self.logger.critical(f\"Unexpected error: {e}\") sys.exit(1) finally: # Clean up resources if hasattr(self, 'pg_connector'): self.pg_connector.close() if hasattr(self, 'ch_connector'): self.ch_connector.close()ClickHouse OptimizationA key part of the solution was optimizing the ClickHouse schema for analytical queries:-- Dimension tables with ReplacingMergeTree engineCREATE TABLE IF NOT EXISTS analytics.dim_advertiser( advertiser_id UInt32, name String, updated_at DateTime, created_at DateTime) ENGINE = ReplacingMergeTree(updated_at)ORDER BY (advertiser_id);-- Fact tables with partitioningCREATE TABLE IF NOT EXISTS analytics.fact_impressions( impression_id UInt32, campaign_id UInt32, event_date Date, event_time DateTime, created_at DateTime) ENGINE = MergeTree()PARTITION BY toYYYYMM(event_date)ORDER BY (campaign_id, event_date);Materialized Views for KPIsI created several materialized views to pre-calculate common KPIs:-- Materialized view for campaign CTRCREATE MATERIALIZED VIEW IF NOT EXISTS analytics.mv_campaign_ctr( campaign_id UInt32, campaign_name String, advertiser_name String, impressions UInt64, clicks UInt64, ctr Float64)ENGINE = SummingMergeTree()ORDER BY (campaign_id)POPULATE ASSELECT c.campaign_id, c.name AS campaign_name, a.name AS advertiser_name, COUNT(DISTINCT i.impression_id) AS impressions, COUNT(DISTINCT cl.click_id) AS clicks, COUNT(DISTINCT cl.click_id) / COUNT(DISTINCT i.impression_id) AS ctrFROM dim_campaign cJOIN dim_advertiser a ON c.advertiser_id = a.advertiser_idLEFT JOIN fact_impressions i ON c.campaign_id = i.campaign_idLEFT JOIN fact_clicks cl ON c.campaign_id = cl.campaign_idGROUP BY c.campaign_id, c.name, a.name;TestingI implemented comprehensive testing with pytest to ensure the reliability of the ETL pipeline: Unit tests for individual components Integration tests for the end-to-end pipeline Schema tests for database schema validationFor example, the unit tests for the Transformer component:@pytest.mark.unitclass TestDataTransformer: \"\"\"Tests for DataTransformer.\"\"\" def test_transform_advertisers(self): \"\"\"Test transforming advertiser data.\"\"\" now = datetime.now() input_data = [(1, 'Advertiser A', now, now)] transformer = DataTransformer() result = transformer.transform_advertisers(input_data) assert result == [(1, 'Advertiser A', now, now)]Deployment with DockerI containerized the entire solution using Docker to ensure consistent operation:FROM python:3.12-slimWORKDIR /app# Install dependenciesCOPY requirements.txt .RUN pip install --no-cache-dir -r requirements.txt# Copy ETL filesCOPY . .# Run the ETL scriptCMD [\"python\", \"main.py\"]And orchestrated the services with Docker Compose:services: # PostgreSQL postgres: image: postgres:17 container_name: psql_source env_file: .env ports: - \"6543:5432\" volumes: - postgres_data:/var/lib/postgresql/data # ClickHouse clickhouse: image: clickhouse/clickhouse-server container_name: ch_analytics env_file: .env ports: - \"8124:8123\" - \"9001:9000\" volumes: - clickhouse_data:/var/lib/clickhouse # ETL Service etl: build: context: ./etl dockerfile: Dockerfile.etl container_name: adtech_etl restart: unless-stopped depends_on: postgres: condition: service_healthy clickhouse: condition: service_healthy env_file: .env volumes: - ./etl:/appCI/CD PipelineTo ensure code quality, I set up a GitHub Actions workflow:name: CIon: push: branches: [ main, dev ] pull_request: branches: [ main, dev ]jobs: lint: name: Code Quality Checks runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Python 3.12 uses: actions/setup-python@v5 with: python-version: '3.12' - name: Install dependencies run: | python -m pip install --upgrade pip pip install ruff==0.11.0 - name: Lint with ruff run: python -m ruff check etl tests test: name: Unit Tests runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Python 3.12 uses: actions/setup-python@v5 with: python-version: '3.12' - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r etl/requirements.txt pip install pytest==8.3.5 - name: Run unit tests run: python -m pytest -xvs -m \"unit\"Scalability ApproachesAs data volumes grow in advertising platforms, the ETL pipeline must scale accordingly. Here are the key scalability approaches I implemented and recommend for further expansion:1. Horizontal Scaling with Distributed ProcessingThe current architecture can be enhanced for horizontal scalability by implementing:class DistributedETLPipeline(ETLPipeline): \"\"\"Distributed version of the ETL pipeline that supports partitioned processing.\"\"\" def __init__(self, extractor, transformer, loader, partition_count=4): super().__init__(extractor, transformer, loader) self.partition_count = partition_count def partition_data(self, table_name, date_field, partition_key): \"\"\"Create logical partitions for parallel processing.\"\"\" partition_ranges = [] # Calculate partition boundaries based on time or ID ranges return partition_ranges def run_partitioned_sync(self, table_name, executor): \"\"\"Execute sync operations across multiple partitions in parallel.\"\"\" partitions = self.partition_data(table_name, 'created_at', 'id') futures = [] for partition in partitions: # Submit each partition for parallel execution future = executor.submit(self.sync_partition, table_name, partition) futures.append(future) # Gather results results = [future.result() for future in futures] return sum(results)With this approach, we can use Python‚Äôs concurrent.futures or distributed task queues like Celery to process partitions in parallel across multiple worker nodes.2. Time-Based Batching for High-Volume TablesFor fact tables with millions of daily records (impressions, clicks), implementing time-based batching reduces memory pressure:def sync_impressions_with_batching(self, batch_size=10000, time_window_hours=1) -&gt; int: \"\"\"Sync impressions with time-based batching.\"\"\" total_synced = 0 current_time = self.last_sync['impressions'] end_time = datetime.now() while current_time &lt; end_time: # Calculate next batch window next_window = current_time + timedelta(hours=time_window_hours) if next_window &gt; end_time: next_window = end_time # Extract and process just this time window rows = self.extractor.extract_impressions_by_window(current_time, next_window) if rows: data = self.transformer.transform_impressions(rows) count = self.loader.load_impressions(data) total_synced += count # Move to next window current_time = next_window # Update the final sync timestamp self.last_sync['impressions'] = end_time return total_synced3. Database-Level ScalingAs the system scales, the database architecture can be enhanced:PostgreSQL Scaling: Implement read replicas to isolate the ETL read load from operational writes Use logical replication with PostgreSQL‚Äôs Change Data Capture (CDC) features Consider a replication-based CDC tool like Debezium for near real-time data streamingClickHouse Scaling: Implement a ClickHouse cluster with data sharding across multiple nodes Optimize the sharding key for commonly queried dimensions (e.g., by campaign_id) Implement distributed tables to abstract the sharding complexity:-- Distributed table definitionCREATE TABLE IF NOT EXISTS analytics.dist_fact_impressions AS analytics.fact_impressionsENGINE = Distributed(cluster_name, analytics, fact_impressions, rand());4. Resilient Work Queue ArchitectureFor extreme scale, transition from a scheduled polling approach to an event-driven architecture:# In a message consumer servicedef process_etl_message(self, message): \"\"\"Process a message from the ETL work queue.\"\"\" try: entity_type = message['entity_type'] batch_id = message['batch_id'] time_range = message.get('time_range', None) # Process the specific batch if entity_type == 'impressions': self.etl_pipeline.sync_impressions_batch(batch_id, time_range) elif entity_type == 'clicks': self.etl_pipeline.sync_clicks_batch(batch_id, time_range) # etc. # Acknowledge successful processing self.queue.acknowledge(message['id']) except Exception as e: # Failed processing - handle with retry logic self.queue.retry(message['id']) logger.error(f\"Failed to process ETL message: {e}\")This approach works well with Apache Kafka, RabbitMQ, or cloud-native solutions like AWS SQS/SNS for truly decoupled processing.Optimization TechniquesBeyond the initial implementation, I‚Äôve identified several optimization opportunities:1. Query Optimization for ExtractionImproving extraction performance through optimized queries:def extract_impressions_optimized(self, since: datetime) -&gt; List[Tuple]: \"\"\"Extract impressions with optimized query performance.\"\"\" query = \"\"\" SELECT id, campaign_id, created_at FROM impressions WHERE created_at &gt; %s ORDER BY created_at LIMIT 50000 -- Batch size control \"\"\" return self.db.execute_query(query, (since,))Additionally, I recommend adding appropriate indexes to source tables:-- Add index for incremental extraction performanceCREATE INDEX IF NOT EXISTS idx_impressions_created_at ON impressions(created_at);CREATE INDEX IF NOT EXISTS idx_clicks_created_at ON clicks(created_at);2. Batch Processing and Bulk LoadingImplementing bulk loading operations for ClickHouse significantly improves throughput:def load_impressions_bulk(self, data: List[Tuple]) -&gt; int: \"\"\"Load impression data into ClickHouse using efficient bulk loading.\"\"\" if not data: return 0 # Prepare data for bulk insert formatted_data = [] for imp_id, campaign_id, event_date, event_time, created_at in data: formatted_data.append({ 'impression_id': imp_id, 'campaign_id': campaign_id, 'event_date': event_date, 'event_time': event_time, 'created_at': created_at }) # Execute bulk insert self.db.client.execute( \"INSERT INTO analytics.fact_impressions VALUES\", formatted_data ) return len(data)3. Memory ManagementFor large datasets, implement iterator-based processing to avoid loading entire result sets into memory:def extract_large_dataset(self, since: datetime, batch_size=10000): \"\"\"Extract large datasets using server-side cursors to control memory usage.\"\"\" query = \"\"\" SELECT id, campaign_id, created_at FROM impressions WHERE created_at &gt; %s ORDER BY id \"\"\" with self.db.conn.cursor(name='large_extract_cursor') as cursor: cursor.execute(query, (since,)) while True: batch = cursor.fetchmany(batch_size) if not batch: break yield batch4. Compression and Data Type OptimizationsOptimize ClickHouse storage by carefully selecting compression and data types:-- Optimized fact table with compression and efficient data typesCREATE TABLE IF NOT EXISTS analytics.fact_impressions_optimized( impression_id UInt32 CODEC(Delta, LZ4), campaign_id UInt32 CODEC(Delta, LZ4), event_date Date CODEC(DoubleDelta, LZ4), event_time DateTime CODEC(DoubleDelta, LZ4), created_at DateTime CODEC(DoubleDelta, LZ4)) ENGINE = MergeTree()PARTITION BY toYYYYMM(event_date)ORDER BY (campaign_id, event_date)SETTINGS index_granularity = 8192;5. Parallel Processing for TransformationsImplement parallel transformation processing for CPU-intensive operations:def transform_impressions_parallel(self, rows: List[Tuple]) -&gt; List[Tuple]: \"\"\"Transform impression data using parallel processing.\"\"\" from concurrent.futures import ProcessPoolExecutor # Split data into chunks for parallel processing chunk_size = 10000 chunks = [rows[i:i + chunk_size] for i in range(0, len(rows), chunk_size)] # Process chunks in parallel with ProcessPoolExecutor() as executor: results = list(executor.map(self._transform_impression_chunk, chunks)) # Combine results return [item for sublist in results for item in sublist]Future-Proofing the ArchitectureAs digital advertising continues to evolve, this ETL pipeline can be extended in several ways:1. Streaming Data ProcessingImplement real-time data processing by integrating with streaming platforms:class StreamingETLPipeline: \"\"\"Real-time streaming ETL pipeline for advertising events.\"\"\" def __init__(self, kafka_config, clickhouse_connector): self.consumer = KafkaConsumer( 'adtech.events', bootstrap_servers=kafka_config['bootstrap_servers'], group_id='etl-consumer-group', auto_offset_reset='earliest' ) self.clickhouse = clickhouse_connector def process_streaming_events(self): \"\"\"Process streaming events from Kafka.\"\"\" for message in self.consumer: event = json.loads(message.value.decode('utf-8')) # Process different event types if event['type'] == 'impression': self.process_impression(event) elif event['type'] == 'click': self.process_click(event) def process_impression(self, event): \"\"\"Process impression event and load to ClickHouse.\"\"\" # Transform and load impression data self.clickhouse.execute_query( \"INSERT INTO analytics.fact_impressions VALUES\", [( event['id'], event['campaign_id'], datetime.fromisoformat(event['timestamp']).date(), datetime.fromisoformat(event['timestamp']), datetime.now() )] )2. Machine Learning Feature Store IntegrationExtend the pipeline to support ML feature generation for predictive advertising:class FeatureStoreLoader: \"\"\"Loads transformed data into a feature store for ML applications.\"\"\" def __init__(self, clickhouse_connector, feature_store_client): self.clickhouse = clickhouse_connector self.feature_store = feature_store_client def generate_campaign_features(self): \"\"\"Generate and load campaign performance features.\"\"\" # Extract features from ClickHouse features_data = self.clickhouse.execute_query(\"\"\" SELECT campaign_id, toDate(event_time) AS day, count() AS daily_impressions, sum(case when exists( SELECT 1 FROM analytics.fact_clicks c WHERE c.campaign_id = i.campaign_id AND c.event_date = i.event_date ) then 1 else 0 end) AS daily_clicks, avg(bid) AS avg_bid FROM analytics.fact_impressions i JOIN analytics.dim_campaign c ON i.campaign_id = c.campaign_id GROUP BY campaign_id, day ORDER BY campaign_id, day \"\"\") # Load to feature store self.feature_store.ingest_features( feature_group=\"campaign_daily_performance\", features=features_data )3. Multi-Tenant ArchitectureScale the system to support multiple advertising platforms through tenant isolation:class MultiTenantETLPipeline(ETLPipeline): \"\"\"ETL pipeline with tenant isolation support.\"\"\" def __init__(self, tenant_id, *args, **kwargs): super().__init__(*args, **kwargs) self.tenant_id = tenant_id def extract_tenant_data(self, table, since): \"\"\"Extract data specifically for this tenant.\"\"\" query = f\"\"\" SELECT * FROM {table} WHERE tenant_id = %s AND updated_at &gt; %s \"\"\" return self.extractor.db.execute_query(query, (self.tenant_id, since)) def load_tenant_data(self, table, data): \"\"\"Load data with tenant isolation.\"\"\" # Ensure tenant_id is included in all loaded data tenant_data = [(self.tenant_id,) + row for row in data] return self.loader.load_generic(table, tenant_data)ConclusionBuilding this ETL pipeline for AdTech analytics was a structured exercise in balancing performance, reliability, and maintainability. The modular design allows for easy extension to additional data sources or targets, while the ClickHouse optimizations ensure fast query responses for critical KPIs.While the current implementation meets the immediate needs, I‚Äôve outlined clear paths for scaling and optimizing as data volumes grow. The real power of this architecture lies in its flexibility‚Äîit can evolve from batch processing to streaming, accommodate multi-tenant requirements, and integrate with advanced analytics platforms as business needs change.For organizations starting similar projects, I recommend taking an incremental approach: begin with the core batch ETL functionality, validate the data model with real queries, then progressively enhance with optimizations and scalability features based on actual performance metrics and growth patterns.This architectural pattern has proven highly effective for advertising analytics, where the ability to process billions of events while maintaining query performance is critical to driving business value through data-driven decision making." }, { "title": "Takeaway: Unit Testing of Bash Scripts", "url": "/posts/takeaway-unit-testing-of-bash-scripts/", "categories": "Learning Journey", "tags": "bash, unittest", "date": "2023-09-28 13:40:00 +0200", "snippet": "Bash/Shell is a potent scripting language that lets us communicate with our computer‚Äôs operating system. In many of my everyday tasks, I rely on Bash to carry out Linux commands and create certain logical processes. While Bash‚Äôs capabilities are impressive, it‚Äôs important to note that since it deals with low-level operations within the operating system, it can sometimes escalate minor problems into major ones, without any chance to revert the execution.To ensure the reliability of our scripts for daily business operations, it‚Äôs crucial to incorporate unit testing into our Bash scripts. This practice helps us maintain stability and prevent unexpected issues from affecting our workflow.Briefly write down my takeaway from Chai Feng‚Äôs sharing: https://www.youtube.com/live/-1gB_5dV32U?si=4uICRRG8vDmCKWxE&amp;t=649There are 3 main barriers that prevent us from implementing Bash unit tests. Dependencies, having the exact execution environment can be quite troublesome. The script could have potential side effects and can be tricky to set up or tear down. Command execution is the aim of the script, therefore it‚Äôs hard to foresee the result. Execution/validation is not fast enough.Let‚Äôs revisit the concept of script‚Äôs unit tests. What exactly are they?Is it validating whether the function called runs smoothly? Or to validate the execution logic of the indicated script? It doesn‚Äôt matter running commands manually or by a script if the functions resulting errors. So the concept to implement scripts‚Äô unit testing should be to validate the logic, instead of testing if the function can be run.So what should we consider next? The methodology in Bash is essentially as same as other languages unit testing. Don‚Äôt bother executing the commands in the PATH because they are external dependencies of the script. Each test instance is independent. Different platforms, different environments, same results. The validation action we expect is to execute the expected command, and send the expected parameters.There are 5 things that determine the execution of commands. In order, they are aliases, keywords, functions, built-in procedures, and external procedures. The most important thing to consider about is how to simulate the command?The testing framework for bash scripts: https://github.com/bach-sh/bach" }, { "title": "Learning and Takeaways from Kubesimplify Workshop", "url": "/posts/learning-and-takeaways-from-kubesimplify-workshop/", "categories": "Learning Journey", "tags": "devops, linux, docker, k8s", "date": "2022-08-07 00:03:00 +0200", "snippet": "This post will keep the learning process and takeaways from the Kubesimplify workshops held by Saiyam Pathak. The motivation for me to catch up on this DevOps topic is to systematically learning by practising for DevOps mindset, in order to achieve double blade stacks, facilitating Data Engineering tasks and projects via DevOps approaches.Workshops List Linux &amp; Docker Fundamentals (Ongoing) Kubernetes 101 GitOps With ArgoCD Kubernetes Security 101 Kubernetes TroubleshootingLinux &amp; Docker FundamentalsInstructor: Chad M. Crowell Lecture&amp;Workshop recording Learning resourceLinux fundamentalI‚Äôve been familiar with most of the commands introduced in this session. I still learnt something new because my learning before wasn‚Äôt systematical enough. The takeaways for me in this session are listed below: Knowing the naming and how Linux filesystem works in a clear picture. Pressing control + r in the prompt is able to search historical executions. Under z-shell, there‚Äôs a way fancier drop-down list for a more intuitive search. Using Command man to look up the manual of each command. -h isn‚Äôt supported for every command, learnt how to use man is a great finding for me. Linux commands are case-sensitive. Simple command without switch on editor echo 'var=\"something\"' &gt; file # To overwrite the fileecho \"var=\"something\"' &gt;&gt; file # To append to the file Create an intermediate directory with -p flag while using mkdir, for example mkdir test/sub-test/error # The prompt would pop out errormkdir -p test/sub-test/correct # Successful execution chmod commands usage https://chmodcommand.com/chmod-600/To be continued‚Ä¶" }, { "title": "Implementing Fivetran Data Source Connector with AWS Lambda", "url": "/posts/implementing-fivetran-data-source-connector-with-aws-lambda/", "categories": "Data Engineering", "tags": "aws, lambda, fivetran, etl", "date": "2022-05-28 22:47:00 +0200", "snippet": "Basic background of AWS LambdaOfficial developer guide from AWSAWS Lambda is a serverless, event-driven compute service that lets you run code for virtually any type of application or backend service without provisioning or managing servers. In GCP and Azure, we can implement the same idea via Cloud Functions and Azure Funtions.Why is it necessary to understand Lambda to build Fivetran connectors?Fivetran is a reliable data pipeline platform for business users to connect their data source in a convenient way. It provides tons of connectors and integrations for user to choose, like marketing tools, modern databases, etc. Although Fivetran supports amounts of external APIs and data sources integration in default, some of the external APIs we needin reality which doesn‚Äôt be supported by Fivetran directly.If you are using AWS, then Lambda stands out at this moment! It allows us to write custom integration functions in Python, Node.js, etc. to approach data integration which not directly supported by Fivetran. In the following use case, connector/ETL was built in Python, connecting an API and Snwoflake. But this article would only focus on the less mentioned parts of the official Fivetran documentation. The basic architecture is shown in the figure below. PrerequisiteFollow the instruction from Fivetran to setup the configuration, click hereLambda‚Äôs sample function from Fivetran‚Äôs documentimport jsondef lambda_handler(request, context): # Fetch records using api calls (insertTransactions, deleteTransactions, newTransactionCursor) = api_response(request['state'], request['secrets']) # Populate records in insert insert = {} insert['transactions'] = insertTransactions delete = {} delete['transactions'] = deleteTransactions state = {} state['transactionsCursor'] = newTransactionCursor transactionsSchema = {} transactionsSchema['primary_key'] = ['order_id', 'date'] schema = {} schema['transactions'] = transactionsSchema response = {} # Add updated state to response response['state'] = state # Add all the records to be inserted in response response['insert'] = insert # Add all the records to be marked as deleted in response response['delete'] = delete # Add schema defintion in response response['schema'] = schema # Add hasMore flag response['hasMore'] = 'false' return response\tdef api_response(state, secrets): # your api call goes here insertTransactions = [ {\"date\":'2017-12-31T05:12:05Z', \"order_id\":1001, \"amount\":'$1200', \"discount\":'$12'}, {\"date\":'2017-12-31T06:12:04Z', \"order_id\":1001, \"amount\":'$1200', \"discount\":'$12'}, ] deleteTransactions = [ {\"date\":'2017-12-31T05:12:05Z', \"order_id\":1000, \"amount\":'$1200', \"discount\":'$12'}, {\"date\":'2017-12-31T06:12:04Z', \"order_id\":1000, \"amount\":'$1200', \"discount\":'$12'}, ] return (insertTransactions, deleteTransactions, '2018-01-01T00:00:00Z')Multi-pages response implementationFivetran will stop the request if it gets a response has an attribute hasMore which equals 'false'response['hasMore'] = 'false'Which means, more than 1 pages response should be able to switch the value by a pointer. false should be able to altered to make Fivetran know that the request hasn‚Äôt finished, the pointer should be updated as well once all pages are done. I‚Äôm sharing my implementation below to meet this requirement.import datetimeimport asynciofrom services import processor, cursor_formatterdef handler(event, context): \"\"\" Lambda function handler to handle output format \"\"\" loop = asyncio.get_event_loop() insertLoanApplication, insertApplicants, insertAdvisors, state = \\ loop.run_until_complete(api_response(event['state'], event['secrets'])) if not state['moreData']: loop.close() insert = { 'loan_applications': insertLoanApplication, 'applicants': insertApplicants, 'advisors': insertAdvisors } schema_loan_applications = {'primary_key': ['id']} schema_applicants = {'primary_key': ['loan_application_id', 'id']} schema_advisors = {'primary_key': ['id']} schema = { 'loan_applications': schema_loan_applications, 'applicants': schema_applicants, 'advisors': schema_advisors } return { 'state': state if state['moreData'] else {'cursor': cursor_formatter(datetime.datetime.now()), 'page': 0}, 'insert': insert, 'schema': schema, 'hasMore': 'false' if not state['moreData'] else 'true' }async def api_response(state, secrets): \"\"\" Main function to call indicated API :param state: Fivetran anchor for indexing usage, default None :param secrets: The secret you would like to use to call the API :return: API responses \"\"\" try: cursor_value, page = state['cursor'], state[\"page\"] except KeyError: cursor_value, page = cursor_formatter(datetime.datetime.now()), 0 page += 1 insertLoanApplication, insertApplicant, insertAdvisor, moreData = await processor(secrets, page, cursor_value) state = {'cursor': cursor_value, 'page': page, 'moreData': moreData} return insertLoanApplication, insertApplicant, insertAdvisor, stateFirst, we can see in api_request function, state is assigned by request format, empty dictionary object in default, we can assign any pointer we need for requesting API, see here to check the detail.We retrieve the cursor and page at the beginning, cursor is for locating the timestamp of each response whether we‚Äôve done already, and page is literally for locating which page we are at. Fivetran can tell if this is an initial request, or if it is a request that is still pending and should be continued.try: cursor_value, page = state['cursor'], state[\"page\"]except KeyError: cursor_value, page = cursor_formatter(datetime.datetime.now()), 0After processing, we will get a function return value moreData for the Lambda handler to continue or stop the request.state = {'cursor': cursor_value, 'page': page, 'moreData': moreData}In this example, we have the return value from the handler to present continuing or stopping. Else we have no more new response from API, returning a timestamp and reset the page value to 0 for the next round requesting.return { 'state': state if state['moreData'] else {'cursor': cursor_formatter(datetime.datetime.now()), 'page': 0}, 'insert': insert, 'schema': schema, 'hasMore': 'false' if not state['moreData'] else 'true'}ConclusionSince the usage context is relatively small, Fivetran does not have document that describes how to request a multi-page response from API requesting, so we use several pointers to implement the requirements alone, which still quite fits actually.If you‚Äôve noticed, I‚Äôve also designed asynchronous requests for this purpose to speed up request efficiency, but this also creates a burden on the API server, I‚Äôll share how to optimize requests in a later post." }, { "title": "Useful gadget sharing - cron-job.org", "url": "/posts/useful-gadget-sharing-cron-job-org/", "categories": "Sharing", "tags": "heroku, cron", "date": "2021-04-30 02:00:00 +0200", "snippet": "Useful gadget sharing - cron-job.orgDue to initiating to maintain my side projects which have done before, I started investigating any pain points that needed to be improved or could be divided into small projects.StoryI want to introduce a really cool, simple but useful gadget for you, cron-job.org.The context was that I had a side project which deployed on Heroku. Heroku is a really convenient platform that provides a basic deployment environment for someone who wants to build their website or front-end app interface.In short, Heroku free tier is quite sufficient for me. But your app is put to down after 30 mins of inactivity, and will need around 5‚Äì10 seconds to wake up the app again. If your app cannot tolerate that in any circumstance, you may want to try some approaches to avoid your app sleep again but still free. Pinging your app on a set interval might be an economy way to achieve it.Approach Make sure you have a simple GET request to your app homepage. For example, if your app is hosted at your-app.herokuapp.com, make sure you have a GET method handling ‚Äú/‚Äù route. Or, have a GET method at some other route. Once that‚Äôs completed, we can either handle pinging our own app manually or automate the process. The Heroku free tier requires applications to sleep a minimum of seven hours every day. Unverified accounts get 550 of free dyno hours per month, which is equal to almost 7 hours of sleep time for a month of 31 days. To be on the safe side, we‚Äôll go with seven hours of sleep. The example GET function writing in Python/Flask@bp.route('/ping', methods=['GET'])def ping(): res = { \"name\": \"CV parser\", \"requested_at\": datetime.datetime.now(), \"status\": \"ok\" } return jsonify(res)Using cronjob.org to keep your app alive Signup on https://cron-job.org/en/ Verify your email once the signup is done. Log in and click on the Cronjobs tab. And click on ‚ÄúCreate cronjob‚Äù. Fill in the details. Enter URL according to your GET method. For example, my URL will be https://you.herokuapp.com/pingSchedule the execution time from 8 to 23 every 30 minutes every day You can also check the execution detail in dashboard " }, { "title": "Run your own Apache Spark jobs in AWS EMR and S3", "url": "/posts/run-your-own-apache-spark-jobs-in-aws-emr-and-s3/", "categories": "Data Engineering", "tags": "aws, datalake, pipeline", "date": "2021-02-01 01:00:00 +0100", "snippet": "Run your own Apache Spark jobs in AWS EMR and S3Recently, I participated in Udacity‚Äôs Nanodegree Program for Data Engineers. It‚Äôs kinda like to review what I did past and refresh some tech stacks‚Äô knowledge for me in the Data Engineering field. Today, I‚Äôm gonna writing this article to avoid you spending extra efforts to run a Spark job or other Hadoop eco-systems jobs on AWS.This idea comes from the Data Lake demo below, you can find the part I tried to apply through Shell script and Makefile between [S3-&gt;EMR-&gt;S3] in the figure. The PySpark script I used reference from the project in Course Data Lake, click the photo to check detail.Before we start, make sure you‚Äôve already done with registering an AWS account and downloading the credential on AWS console(the easiest way, you bet).EMR workflowFirst off, let us have a look at the EMR workflow, click the photo to check detail.What we want to do, is to make sure the work of setting up and launching a cluster, adding working steps, creating log files, and terminating the cluster as simple as we can. AWS CLI(command-line interface) builds an easy interface and functional usage for operating various services on AWS. But we are eager to make entire management much simpler and flexible to check the status, even put it aside until the job is finished.Of course, you have other choices like SDK for different languages, like Python SDK or Java SDK if you like. In my experiences, every SDK sometimes use a different version of libraries or packages, you should be aware of the supporting version on each service you choose.Idea and PracticeWe can choose the easy-checking cli command for pipeline, such as creating-cluster, add-steps, and terminate-clusters.The cli command from document is shown below.aws emr create-cluster \\--name \"My First EMR Cluster\" \\--release-label emr-5.31.0 \\--applications Name=Spark \\--ec2-attributes KeyName=myEMRKeyPairName \\--instance-type m5.xlarge \\--instance-count 3 \\--use-default-rolesSet the command as a variable to load in the next step, add 2 additional arguments --query '[ClusterId]', --output text to change the output format and query the information we need, reference the variable cluster_id in the Shell scripts as below.cluster_id=\"$(aws emr create-cluster --name SparkifyEMR \\--release-label emr-5.31.0 \\--log-uri \"s3://$clustername/logs/\" \\--applications Name=Spark \\--ec2-attributes KeyName=\"$keyname\" \\--instance-type m5.xlarge \\--instance-count 3 \\--use-default-roles \\--query '[ClusterId]' \\--output text)\"echo -e \"\\nClusterId: $cluster_id\"Set the second variable step_id and query it.step_id=\"$(aws emr add-steps \\--cluster-id \"$cluster_id\" \\--steps \"Type=Spark,Name=SparkifyETL,ActionOnFailure=CONTINUE,\\Args=[s3://$clustername/run_etl.py,--data_source,$clustername/data/,--output_uri,$clustername/output/]\" \\--query 'StepIds[0]' \\--output text)\"echo -e \"\\nStepId: $step_id\"Set a while loop to check Spark jobs is finished or not.describe_step=\"$(aws emr describe-step --cluster-id \"$cluster_id\" \\ --step-id \"$step_id\" --query 'Step.Status.State' --output text)\"while true; do if [[ $describe_step = 'PENDING' ]]; then echo \"Cluster creating... Autocheck state 2 mins later\" sleep 120 elif [[ $describe_step = 'RUNNING' ]]; then echo \"Job running... Autocheck state 30 seconds later\" sleep 30 elif [[ $describe_step = 'FAILED' ]]; then echo \"Job failed\" break else echo \"Job completed\" break fidoneAfter finishing all jobs, terminate the cluster of the jobs complete without error.terminate_cluster=\"$(aws emr terminate-clusters \\--cluster-ids \"$cluster_id\" \\--query 'Cluster.Status.State' \\--output text)\"while true; do if [[ $terminate_cluster != 'TERMINATED' ]]; then echo \"Cluster terminating...\" sleep 15 else echo \"Cluster terminated\" fidoneConclusionAnyway, this script brings you a convenient approach to run one-time Spark jobs in AWS EMR Cluster. You can also check the all lines in the script here for reference.if you‚Äôd like to try it yourself, follow the tutorial from AWS official documents to build and manage your EMR clusters and jobs." }, { "title": "How to train a customized Name Entities Recognition (NER) model based on spaCy pre-trained model", "url": "/posts/how-to-train-a-customized-name-entities-recognition-ner-model-based-on-spacy-pre-trained-model/", "categories": "Learning Journey", "tags": "ner, spacy, nlp", "date": "2020-03-04 01:00:00 +0100", "snippet": "How to train a customized Name Entities Recognition (NER) model based on spaCy pre-trained modelThere are a bunch of online resources to teach you how to train your own NER model by spaCy, so I will attach some links for reference and skip this part.spaCy: Training the named entity recognizerConvert SpaCy training data to Dataturks NER JSON outputCustom Named Entity Recognition Using spaCyBack to our main point of this article, and let‚Äôs check the following code. I‚Äôm trying to train a customized NER model and retain the other pipeline in spaCy, i.e. tagger and parser. You can also add your own pipeline into the model as well.Write a function as below, and load the model if you‚Äôve pointed it out. I used default pre-trained model en_core_web_sm here.def train_spacy(self, training_data, testing_data, dropout, display_freq=1, output_dir=None, new_model_name=\"en_model\"): # create the built-in pipeline components and add them to the pipeline # nlp.create_pipe works for built-ins that are registered with spaCy # create blank Language class if self.model: nlp = spacy.load(self.model) else: nlp = spacy.blank('en')The following is just like the tutorial from spaCy official website, create the pipeline or grab it, then add labels from the training data you‚Äôve annotated already. if 'ner' not in nlp.pipe_names: ner = nlp.create_pipe('ner') nlp.add_pipe(ner, last=True) else: ner = nlp.get_pipe('ner') # add labels for _, annotations in training_data: for ent in annotations.get('entities'): ner.add_label(ent[2])And now, let‚Äôs training. The initial loss as 100000, and I set up the early stop to avoid overfitting. # get names of other pipes to disable them during training other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] with nlp.disable_pipes(*other_pipes): # only train NER optimizer = nlp.begin_training() print(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Training the model &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\\n\") losses_best = 100000 early_stop = 0Use stochastic gradient descent as optimizer with mini-batch configuration, get more insights here. for itn in range(self.n_iter): print(f\"Starting iteration {itn + 1}\") random.shuffle(training_data) losses = {} batches = minibatch(training_data, size=compounding(4., 32., 1.001)) for batch in batches: text, annotations = zip(*batch) nlp.update( text, # batch of texts annotations, # batch of annotations drop=dropout, # dropout - make it harder to memorise data sgd=optimizer, # callable to update weights losses=losses) if itn % display_freq == 0: print(f\"Iteration {itn + 1} Loss: {losses}\") if losses[\"ner\"] &lt; losses_best: early_stop = 0 losses_best = int(losses[\"ner\"]) else: early_stop += 1 print(f\"Training will stop if the value reached {self.not_improve}, \" f\"and it's {early_stop} now.\\n\") if early_stop &gt;= self.not_improve: break print(\"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Finished training &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\")After training, dump the model as a binary data to your on-premises disk. Put the last 10 lines inside the with nlp.disable_pipes(*other_pipes): will save the only ner pipeline. Otherwise, the entire model will be saved, that‚Äôs exactly what I need. if output_dir: path = output_dir + f\"en_model_ner_{round(losses_best, 2)}\" else: path = os.getcwd() + f\"/lib/inactive_model/en_model_ner_{round(losses_best, 2)}\" os.mkdir(path) if testing_data: self.validate_spacy(model=nlp, data=testing_data) with nlp.use_params(optimizer.averages): nlp.meta[\"name\"] = new_model_name bytes_data = nlp.to_bytes() lang = nlp.meta[\"lang\"] pipeline = nlp.meta[\"pipeline\"] model_data = dict(bytes_data=bytes_data, lang=lang, pipeline=pipeline) with open(path + '/model.pkl', 'wb') as f: pickle.dump(model_data, f)You can load the model by the following function to load your model by your specified path on your disk.def load_model(model_path): with open(model_path + '/model.pkl', 'rb') as f: model = pickle.load(f) nlp = spacy.blank(model['lang']) for pipe_name in model['pipeline']: pipe = nlp.create_pipe(pipe_name) nlp.add_pipe(pipe) nlp.from_bytes(model['bytes_data']) return nlp" }, { "title": "Connect PostgreSQL in docker container with Azure Data Studio", "url": "/posts/connect-postgresql-in-docker-container-with-azure-data-studio/", "categories": "Learning Journey", "tags": "azure-data-studio", "date": "2020-02-05 01:00:00 +0100", "snippet": "Connect PostgreSQL in docker container with Azure Data StudioAzure Data Studio is a cool product that can easily connect MySQL (if you already installed in your system) and show what in your database look like. What if we use PostgresSQL instead of MySQL, how should we start?The 1st step, I decided to pull Postgres‚Äôs docker image but not to install at root.$ docker pull postgresSecond, run the container, remember to set the port for local connecting (the document didn‚Äôt mention it)$ docker run --name postgres-docker -e POSTGRES_PASSWROD=secret_password -p 5432:5432 -d postgresthen you will get container id, also can check it by docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba01d334f4db postgres \"docker-entrypoint.s‚Ä¶\" 37 minutes ago Up 37 minutes 0.0.0.0:5432-&gt;5432/tcp postgres-dockerAfter running the container, we could start the next step, set up the connection in Azure Data Studio.1. Install PostgreSQL plug-in.Choose the extension‚Äôs icon and search Postgres then install it. 2. Click new connection, choose PostgreSQL as your database. 3. Enter the required information. 4. Remember to set port in advanced setting.Enter the ip and port as same as you assigned to docker container. 5. Connect and start to work in the database. So far, the most important part is that remember to assign port when you run the docker container and enter the correct connection details, then it can keep your life much easier." }, { "title": "Setting both Celery and Flask inside the docker-compose", "url": "/posts/setting-both-celery-and-flask-inside-the-docker-compose/", "categories": "Learning Journey", "tags": "celery, flask, docker", "date": "2020-02-04 01:00:00 +0100", "snippet": "Setting both Celery and Flask inside the docker-composeDue to the issue I need to resolve is that put the heavy task to background, then run it periodically or asynchronously. And the most important thing is that have to support Python.I found Celery, the excellent tool to implement distributed task queue by Python, can easily deal with request using asynchronous designing.It seems like a perfect solution for my issue, however, there has still a new problem arised, what is I need to focus on processing the task ran by Flask and packed in docker and running by docker compose, it is difficult to find all of these resources at the same time on the Internet.So let‚Äôs started!My article is referenced from the following resources: Asynchronous Tasks with Flask and Redis Queue The web-service for extracting dominant color from images. Using Celery with Flask Flask + Celery tutorial (Mandarin resource) Task schduling with Celery (Mandarin resource)App Structure DisplayingFlask_Celery_example|‚îú‚îÄ‚îÄ api| ‚îú‚îÄ‚îÄ __init__.py| ‚îú‚îÄ‚îÄ app.py | ‚îú‚îÄ‚îÄ celery_app.py| ‚îú‚îÄ‚îÄ config.py| ‚îú‚îÄ‚îÄ Dockerfile| ‚îî‚îÄ‚îÄ requirements.txt|‚îú‚îÄ‚îÄ docker-compose.yml‚îî‚îÄ‚îÄ README.mdSet up your Redis and Celery worker in docker-compose, the breaking point is to set up the celery worker‚Äôs name well.version: \"3\"services: web: container_name: web build: ./api ports: - \"5000:5000\" links: - redis depends_on: - redis environment: - FLASK_ENV=development volumes: - ./api:/app redis: container_name: redis image: redis:5.0.5 hostname: redis worker: build: context: ./api hostname: worker entrypoint: celery command: -A celery_app.celery worker --loglevel=info volumes: - ./api:/app links: - redis depends_on: - redisWhat the Dockerfile in this example looks like.FROM python:3.6.5WORKDIR /appCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtCOPY . .EXPOSE 5000CMD [ \"python\", \"./app.py\" ]Initialize Celery worker and create asynchronous or scheduled task in celery_app.pyimport configfrom celery import Celery# Initialize Celerycelery = Celery( 'worker', broker=config.CeleryConfig.CELERY_BROKER_URL, backend=config.CeleryConfig.CELERY_RESULT_BACKEND)@celery.task()def func1(arg): ... return ...Run task by Flask and check its status in app.pyimport configfrom celery_app import func1from flask import Flask, request, jsonify, url_for, redirect# Your API definitionapp = Flask(__name__)app.config.from_object('config.FlaskConfig')@app.route('/', methods=['POST'])def longtask(): task = func1.delay(arg) return redirect(url_for('taskstatus', task_id=task.id)) @app.route('/status/&lt;task_id&gt;')def taskstatus(task_id): task = func1.AsyncResult(task_id) if task.state == 'PENDING': time.sleep(config.SERVER_SLEEP) response = { 'queue_state': task.state, 'status': 'Process is ongoing...', 'status_update': url_for('taskstatus', task_id=task.id) } else: response = { 'queue_state': task.state, 'result': task.wait() } return jsonify(response)if __name__ == '__main__': app.run()After finishing the all setting, we can run the script on terminal below to create container and run it.$ cd Flask_Celery_example$ docker-compose build$ docker-compose runConclusionThe bottleneck in this case is that I could run the Flask, Redis, Celery separately in docker-compose at first, but if I want to run it with only one script then it failed. I had tried and error lots of times to finding the breaking point, the correct script to run Celery in docker-compose. Everything was clear after throughing this bottleneck.Thanks for reading my first article! Leave your message below my Facebook post if there‚Äôs still any further questions. See you then." } ]
